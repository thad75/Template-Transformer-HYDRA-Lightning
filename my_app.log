[2022-07-08 16:27:17,043][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-08 16:27:17,044][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-08 16:27:17,199][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 00:47:28,201][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:47:28,201][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 00:48:50,741][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:48:50,742][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 00:50:52,902][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:50:52,913][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 10:20:34,133][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 10:20:34,143][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:22:15,438][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:22:15,440][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:23:23,378][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:23:23,379][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:23:23,520][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:26:09,031][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:26:09,033][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:26:09,154][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:35:18,283][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:35:18,295][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:35:18,506][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:35:35,115][train][INFO] - Instantiating model <models.fast_detr>
[2022-07-11 11:37:25,532][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:37:25,533][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:37:25,604][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:37:30,897][train][INFO] - Instantiating model <models.FastDETR>
[2022-07-11 11:43:07,256][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:43:07,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:43:07,327][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:43:11,154][train][INFO] - Instantiating model <models.FastDETR>
[2022-07-11 11:44:01,861][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:44:01,861][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:44:01,933][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:44:03,826][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:53:14,574][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:53:14,574][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:53:14,644][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:53:16,424][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:54:05,217][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:54:05,217][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:54:05,288][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:54:06,976][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:54:08,329][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:54:08,337][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:54:08,338][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:09,932][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:57:09,932][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:57:10,002][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:57:12,409][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:57:14,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:57:14,133][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:57:14,134][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:56,559][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:57:56,560][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:57:56,628][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:57:58,317][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:57:59,681][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:57:59,683][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:57:59,684][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:59,685][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 11:57:59,689][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: False
[2022-07-11 11:57:59,689][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 11:58:27,853][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:58:27,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:58:27,922][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:58:29,606][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:58:30,974][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:58:30,976][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:58:30,977][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:58:30,978][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 11:58:30,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 11:58:30,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 11:58:30,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 11:58:30,984][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 11:58:30,984][train][INFO] - Logging hyperparameters!
[2022-07-11 11:58:30,989][train][INFO] - Starting training!
[2022-07-11 11:58:49,611][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 11:58:51,993][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 11:59:46,195][train][INFO] - Starting testing!
[2022-07-11 11:59:46,688][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 11:59:47,011][train][INFO] - Finalizing!
[2022-07-11 12:00:28,722][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:00:28,722][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:00:28,790][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:00:31,170][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:00:32,723][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:00:32,725][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:00:32,726][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 12:00:32,727][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:00:32,731][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:00:32,731][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:00:32,731][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:00:32,732][train][INFO] - Logging hyperparameters!
[2022-07-11 12:02:02,128][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:02:02,129][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:02:02,197][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:02:03,927][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:02:05,332][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:02:05,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:02:05,335][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:02:05,336][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:02:05,340][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:02:05,340][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:02:05,341][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:02:05,341][train][INFO] - Logging hyperparameters!
[2022-07-11 12:02:05,364][train][INFO] - Starting training!
[2022-07-11 12:02:23,828][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:02:25,420][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:29:40,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:29:40,902][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:29:40,968][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:29:44,602][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:29:46,276][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:29:46,278][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:29:46,279][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:29:46,281][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:29:46,284][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:29:46,285][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:29:46,285][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:29:46,286][train][INFO] - Logging hyperparameters!
[2022-07-11 12:29:46,307][train][INFO] - Starting training!
[2022-07-11 12:30:04,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:30:06,257][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:35:11,949][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:35:11,965][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:35:12,046][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:35:27,517][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:35:29,325][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:35:29,327][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:35:29,328][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:35:29,329][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:35:29,333][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:35:29,333][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:35:29,334][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:35:29,334][train][INFO] - Logging hyperparameters!
[2022-07-11 12:35:29,376][train][INFO] - Starting training!
[2022-07-11 12:35:47,800][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:35:50,165][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:57:58,148][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:57:58,148][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:57:58,216][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:00,986][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:58:18,789][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:58:18,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:58:18,967][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:20,501][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:58:37,177][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:58:37,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:58:37,245][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:38,789][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:59:17,535][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:59:17,535][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:59:17,603][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:59:19,150][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:59:21,266][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:59:21,267][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:59:21,268][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:59:21,270][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:59:21,274][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:59:21,274][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:59:21,274][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:59:21,275][train][INFO] - Logging hyperparameters!
[2022-07-11 12:59:21,295][train][INFO] - Starting training!
[2022-07-11 12:59:39,545][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:59:41,078][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:01:11,145][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:01:11,145][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:01:11,212][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:01:13,045][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:01:15,146][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:01:15,147][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:01:15,148][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:01:15,150][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:01:15,154][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:01:15,154][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:01:15,154][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:01:15,155][train][INFO] - Logging hyperparameters!
[2022-07-11 13:01:15,175][train][INFO] - Starting training!
[2022-07-11 13:01:33,407][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:01:34,940][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:03:06,999][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:03:06,999][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:03:07,066][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:03:08,724][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:10,367][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:04:10,367][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:04:10,435][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:04:12,013][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:48,987][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:04:48,987][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:04:49,054][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:04:50,577][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:52,700][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:04:52,703][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:04:52,703][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:04:52,705][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:04:52,709][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:04:52,709][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:04:52,709][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:04:52,710][train][INFO] - Logging hyperparameters!
[2022-07-11 13:04:52,730][train][INFO] - Starting training!
[2022-07-11 13:05:10,906][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:05:12,432][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:05:33,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:05:33,569][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:05:33,656][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:05:35,355][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:07:03,596][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:07:03,597][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:07:03,662][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:07:05,217][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:14:06,257][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:14:06,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:14:06,324][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:14:07,929][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:15:34,079][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:15:34,079][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:15:34,146][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:15:35,667][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:15:37,782][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:15:37,783][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:15:37,784][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:15:37,786][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:15:37,790][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:15:37,790][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:15:37,790][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:15:37,791][train][INFO] - Logging hyperparameters!
[2022-07-11 13:15:37,811][train][INFO] - Starting training!
[2022-07-11 13:15:55,903][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:15:57,435][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:19:01,580][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:19:01,580][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:19:01,647][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:19:03,235][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:19:05,408][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:19:05,410][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:19:05,410][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:19:05,412][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:19:05,416][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:19:05,416][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:19:05,417][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:19:05,417][train][INFO] - Logging hyperparameters!
[2022-07-11 13:19:05,438][train][INFO] - Starting training!
[2022-07-11 13:19:23,626][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:19:25,161][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:21:43,873][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:21:43,886][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:21:43,953][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:21:45,845][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:21:47,985][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:21:47,987][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:21:47,988][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:21:47,989][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:21:47,993][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:21:47,994][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:21:47,994][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:21:47,994][train][INFO] - Logging hyperparameters!
[2022-07-11 13:21:48,015][train][INFO] - Starting training!
[2022-07-11 13:22:06,227][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:22:07,748][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:23:28,280][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:23:28,280][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:23:28,348][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:23:29,993][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:23:32,343][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:23:32,345][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:23:32,346][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:23:32,347][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:23:32,351][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:23:32,352][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:23:32,352][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:23:32,352][train][INFO] - Logging hyperparameters!
[2022-07-11 13:23:32,373][train][INFO] - Starting training!
[2022-07-11 13:23:50,886][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:23:52,426][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:25:11,911][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:25:11,911][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:25:11,978][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:25:13,499][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:25:15,598][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:25:15,600][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:25:15,601][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:25:15,602][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:25:15,606][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:25:15,606][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:25:15,606][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:25:15,607][train][INFO] - Logging hyperparameters!
[2022-07-11 13:25:15,627][train][INFO] - Starting training!
[2022-07-11 13:25:33,837][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:25:35,346][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:33:42,858][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:33:42,859][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:33:42,926][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:33:45,702][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:33:47,875][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:33:47,877][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:33:47,878][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:33:47,879][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:33:47,883][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:33:47,883][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:33:47,884][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:33:47,884][train][INFO] - Logging hyperparameters!
[2022-07-11 13:33:47,904][train][INFO] - Starting training!
[2022-07-11 13:34:06,060][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:34:07,577][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 14:57:17,719][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 14:57:17,727][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 14:57:17,819][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 14:57:33,381][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 14:57:35,937][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 14:57:35,939][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 14:57:35,940][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 14:57:35,941][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 14:57:35,946][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 14:57:35,947][train][INFO] - Logging hyperparameters!
[2022-07-11 14:57:35,951][train][INFO] - Starting training!
[2022-07-11 14:57:54,489][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 14:57:56,183][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 14:58:54,311][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 14:58:54,311][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 14:58:54,379][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 14:58:56,423][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 14:58:58,475][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 14:58:58,477][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 14:58:58,478][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 14:58:58,479][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 14:58:58,483][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 14:58:58,483][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 14:58:58,484][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 14:58:58,484][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 14:58:58,484][train][INFO] - Logging hyperparameters!
[2022-07-11 14:58:58,489][train][INFO] - Starting training!
[2022-07-11 14:59:17,149][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 14:59:18,702][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:01:02,634][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:01:02,635][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:01:02,702][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:01:06,136][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:01:08,472][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:01:08,474][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:01:08,475][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:01:08,476][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:01:08,480][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:01:08,481][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:01:08,481][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:01:08,481][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:01:08,482][train][INFO] - Logging hyperparameters!
[2022-07-11 15:01:08,486][train][INFO] - Starting training!
[2022-07-11 15:01:27,835][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:01:29,387][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:02:50,543][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:02:50,543][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:02:50,612][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:02:52,251][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:02:54,316][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:02:54,318][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:02:54,319][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:02:54,320][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:02:54,324][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:02:54,325][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:02:54,325][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:02:54,325][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:02:54,326][train][INFO] - Logging hyperparameters!
[2022-07-11 15:02:54,330][train][INFO] - Starting training!
[2022-07-11 15:03:12,728][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:03:14,268][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:04:25,653][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:04:25,654][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:04:25,721][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:04:27,268][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:04:29,304][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:04:29,306][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:04:29,307][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:04:29,309][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:04:29,313][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:04:29,313][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:04:29,313][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:04:29,314][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:04:29,314][train][INFO] - Logging hyperparameters!
[2022-07-11 15:04:29,319][train][INFO] - Starting training!
[2022-07-11 15:04:47,797][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:04:49,347][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:05:32,897][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:05:32,898][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:05:32,963][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:05:34,520][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:05:36,567][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:05:36,569][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:05:36,569][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:05:36,575][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:05:36,579][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:05:36,580][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:05:36,580][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:05:36,580][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:05:36,581][train][INFO] - Logging hyperparameters!
[2022-07-11 15:05:36,585][train][INFO] - Starting training!
[2022-07-11 15:05:54,862][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:05:56,405][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:06:01,471][train][INFO] - Starting testing!
[2022-07-11 15:06:01,978][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:06:02,302][train][INFO] - Finalizing!
[2022-07-11 15:06:49,978][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:06:49,978][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:06:50,047][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:06:52,680][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:06:55,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:06:55,084][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:06:55,085][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:06:55,087][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:06:55,091][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:06:55,091][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:06:55,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:06:55,092][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:06:55,092][train][INFO] - Logging hyperparameters!
[2022-07-11 15:06:55,097][train][INFO] - Starting training!
[2022-07-11 15:07:13,745][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:07:15,294][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:07:20,076][train][INFO] - Starting testing!
[2022-07-11 15:07:20,592][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:07:20,917][train][INFO] - Finalizing!
[2022-07-11 15:39:28,687][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:39:28,688][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:39:28,755][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:39:31,904][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:39:34,699][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:39:34,700][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:39:34,701][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:39:34,703][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:39:34,707][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:39:34,707][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:39:34,707][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:39:34,708][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:39:34,708][train][INFO] - Logging hyperparameters!
[2022-07-11 15:39:34,713][train][INFO] - Starting training!
[2022-07-11 15:39:53,903][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:39:55,442][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:40:00,016][train][INFO] - Starting testing!
[2022-07-11 15:40:00,525][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:40:00,849][train][INFO] - Finalizing!
[2022-07-11 15:53:18,051][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:53:18,052][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:53:18,122][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:53:21,951][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:53:24,472][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:53:24,473][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:53:24,474][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:53:24,476][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:53:24,480][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:53:24,480][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:53:24,480][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:53:24,481][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:53:24,481][train][INFO] - Logging hyperparameters!
[2022-07-11 15:53:24,486][train][INFO] - Starting training!
[2022-07-11 15:53:43,073][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:53:44,616][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:54:49,441][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:54:49,441][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:54:49,510][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:54:51,102][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:54:53,141][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:54:53,143][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:54:53,144][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:54:53,146][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:54:53,150][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:54:53,150][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:54:53,150][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:54:53,151][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:54:53,151][train][INFO] - Logging hyperparameters!
[2022-07-11 15:54:53,156][train][INFO] - Starting training!
[2022-07-11 15:55:11,893][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:55:13,469][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:55:18,316][train][INFO] - Starting testing!
[2022-07-11 15:55:18,833][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:55:19,161][train][INFO] - Finalizing!
[2022-07-11 15:58:56,833][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:58:56,834][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:58:56,903][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:59:00,478][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:59:02,915][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:59:02,916][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:59:02,917][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:59:02,919][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:59:02,923][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:59:02,923][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:59:02,923][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:59:02,924][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:59:02,924][train][INFO] - Logging hyperparameters!
[2022-07-11 15:59:02,929][train][INFO] - Starting training!
[2022-07-11 15:59:21,516][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:59:23,067][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:59:27,920][train][INFO] - Starting testing!
[2022-07-11 15:59:28,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:59:28,759][train][INFO] - Finalizing!
[2022-07-11 16:00:07,619][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:00:07,619][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:00:07,691][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:00:09,452][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:00:11,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:00:11,506][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:00:11,506][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:00:11,508][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:00:11,512][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:00:11,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:00:11,512][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:00:11,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:00:11,513][train][INFO] - Logging hyperparameters!
[2022-07-11 16:00:11,518][train][INFO] - Starting training!
[2022-07-11 16:00:30,184][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:00:31,751][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:00:36,428][train][INFO] - Starting testing!
[2022-07-11 16:00:36,937][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:00:37,263][train][INFO] - Finalizing!
[2022-07-11 16:04:17,896][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:04:17,897][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:04:17,971][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:04:19,640][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:05:50,118][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:05:50,118][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:05:50,188][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:05:51,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:07:09,560][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:07:09,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:07:09,640][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:07:11,243][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:09:05,427][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:09:05,427][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:09:05,502][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:09:07,107][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:09:56,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:09:56,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:09:56,253][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:09:57,844][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:07,987][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:11:07,988][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:11:08,057][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:11:09,659][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:31,651][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:11:31,652][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:11:31,721][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:11:33,320][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:35,452][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:11:35,454][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:11:35,455][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:11:35,456][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:11:35,460][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:11:35,460][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:11:35,461][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:11:35,461][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:11:35,461][train][INFO] - Logging hyperparameters!
[2022-07-11 16:11:35,466][train][INFO] - Starting training!
[2022-07-11 16:11:54,179][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:11:55,766][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:12:00,515][train][INFO] - Starting testing!
[2022-07-11 16:12:01,025][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:12:01,351][train][INFO] - Finalizing!
[2022-07-11 16:17:13,888][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:17:13,888][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:17:13,960][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:17:15,558][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:17:17,585][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:17:17,587][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:17:17,588][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:17:17,590][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:17:17,594][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:17:17,594][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:17:17,594][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:17:17,595][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:17:17,595][train][INFO] - Logging hyperparameters!
[2022-07-11 16:17:17,600][train][INFO] - Starting training!
[2022-07-11 16:17:36,287][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:17:37,858][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:17:42,501][train][INFO] - Starting testing!
[2022-07-11 16:17:43,014][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:17:43,340][train][INFO] - Finalizing!
[2022-07-11 16:21:25,432][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:21:25,433][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:21:25,508][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:21:27,113][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:21:29,180][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:21:29,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:21:29,182][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:21:29,184][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:21:29,188][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:21:29,188][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:21:29,188][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:21:29,189][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:21:29,189][train][INFO] - Logging hyperparameters!
[2022-07-11 16:21:29,194][train][INFO] - Starting training!
[2022-07-11 16:21:47,757][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:21:49,330][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:21:53,907][train][INFO] - Starting testing!
[2022-07-11 16:21:54,417][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:21:54,736][train][INFO] - Finalizing!
[2022-07-11 17:14:24,336][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:14:24,350][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:14:24,445][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:14:39,911][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:14:42,551][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:14:42,552][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:14:42,553][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:14:42,555][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:14:42,559][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:14:42,559][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:14:42,559][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:14:42,560][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:14:42,560][train][INFO] - Logging hyperparameters!
[2022-07-11 17:14:42,565][train][INFO] - Starting training!
[2022-07-11 17:15:01,102][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:03,342][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:15:21,350][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:15:21,350][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:15:21,418][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:15:23,352][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:15:25,501][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:15:25,502][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:15:25,503][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:15:25,506][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:15:25,510][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:15:25,510][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:15:25,510][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:15:25,511][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:15:25,511][train][INFO] - Logging hyperparameters!
[2022-07-11 17:15:25,516][train][INFO] - Starting training!
[2022-07-11 17:15:44,466][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:45,997][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:15:50,998][train][INFO] - Starting testing!
[2022-07-11 17:15:51,509][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:51,830][train][INFO] - Finalizing!
[2022-07-11 17:24:19,602][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:24:19,603][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:24:19,669][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:24:21,950][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:24:24,332][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:24:24,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:24:24,335][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:24:24,336][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:24:24,340][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:24:24,340][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:24:24,341][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:24:24,341][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:24:24,341][train][INFO] - Logging hyperparameters!
[2022-07-11 17:24:24,346][train][INFO] - Starting training!
[2022-07-11 17:24:42,774][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:24:44,291][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:24:48,859][train][INFO] - Starting testing!
[2022-07-11 17:24:49,359][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:24:49,765][train][INFO] - Finalizing!
[2022-07-11 22:41:23,502][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 22:41:23,517][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 22:41:23,618][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 22:41:41,997][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 22:41:44,745][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 22:41:44,746][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 22:41:44,747][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 22:41:44,749][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 22:41:44,753][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 22:41:44,753][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 22:41:44,753][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 22:41:44,754][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 22:41:44,754][train][INFO] - Logging hyperparameters!
[2022-07-11 22:41:44,759][train][INFO] - Starting training!
[2022-07-11 22:42:03,262][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 22:42:05,448][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 22:42:11,012][train][INFO] - Starting testing!
[2022-07-11 22:42:11,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 22:42:11,852][train][INFO] - Finalizing!
[2022-07-11 23:12:04,540][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 23:12:04,540][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 23:12:04,609][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 23:12:14,882][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 23:12:17,539][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 23:12:17,541][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 23:12:17,542][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 23:12:17,544][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 23:12:17,548][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 23:12:17,548][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 23:12:17,548][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 23:12:17,549][train][INFO] - Logging hyperparameters!
[2022-07-11 23:12:17,610][train][INFO] - Starting training!
[2022-07-11 23:12:35,973][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 23:12:37,514][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 23:13:23,875][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 23:13:23,876][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 23:13:23,944][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 23:13:25,666][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 23:13:27,827][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 23:13:27,828][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 23:13:27,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 23:13:27,831][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 23:13:27,835][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 23:13:27,835][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 23:13:27,835][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 23:13:27,836][train][INFO] - Logging hyperparameters!
[2022-07-11 23:13:27,856][train][INFO] - Starting training!
[2022-07-11 23:13:46,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 23:13:48,052][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 00:23:33,965][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 00:23:33,984][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 00:23:34,103][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 00:23:56,099][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 00:23:59,031][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 00:23:59,032][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 00:23:59,033][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 00:23:59,034][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 00:27:36,331][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 00:27:36,332][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 00:27:36,419][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 00:27:53,064][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 00:27:55,638][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 00:27:55,640][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 00:27:55,640][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 00:27:55,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,659][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,660][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,654][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,660][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,648][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,650][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 00:27:55,663][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 00:27:55,666][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,667][train][INFO] - Logging hyperparameters!
[2022-07-12 00:27:55,833][train][INFO] - Starting training!
[2022-07-12 00:27:55,834][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 00:27:56,672][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 00:27:56,672][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,673][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,382][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 00:46:52,627][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,628][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,628][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,629][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,630][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,632][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,647][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,644][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 09:31:51,722][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 09:31:51,747][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 09:31:51,848][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 09:32:09,737][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 09:32:12,532][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 09:32:12,534][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 09:32:12,534][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 09:32:12,536][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 09:32:12,538][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 09:32:12,539][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,539][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 09:32:12,539][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,540][train][INFO] - Logging hyperparameters!
[2022-07-12 09:32:12,540][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 09:32:12,544][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,545][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 09:32:12,549][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,550][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 09:32:12,553][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,554][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 09:32:12,554][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,555][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 09:32:12,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,564][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 09:32:12,651][train][INFO] - Starting training!
[2022-07-12 09:32:12,652][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 09:32:13,540][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 09:32:13,541][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 09:32:13,546][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 09:32:13,551][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 09:32:13,555][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 09:32:13,556][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 09:32:13,564][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,572][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 09:32:13,575][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,575][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,576][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,577][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,581][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,276][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 11:13:22,416][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 11:13:22,451][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 11:13:22,553][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 11:13:40,121][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 11:13:42,798][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 11:13:42,800][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 11:13:42,801][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 11:13:42,802][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 11:13:42,804][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 11:13:42,805][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 11:13:42,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,806][train][INFO] - Logging hyperparameters!
[2022-07-12 11:13:42,806][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 11:13:42,807][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,807][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,808][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 11:13:42,808][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,808][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 11:13:42,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 11:13:42,815][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,815][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,816][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 11:13:42,816][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 11:13:42,972][train][INFO] - Starting training!
[2022-07-12 11:13:42,973][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 11:13:43,806][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 11:13:43,807][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 11:13:43,809][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 11:13:43,809][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 11:13:43,810][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 11:13:43,816][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 11:13:43,817][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 11:13:43,821][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 11:13:43,821][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,821][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 11:13:43,827][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,829][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,829][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,830][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,137][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:11:37,224][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:11:37,245][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:11:37,439][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:11:53,591][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:12:52,187][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:12:52,188][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:12:52,258][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:12:54,478][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:12:57,326][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:12:57,380][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:12:57,381][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:12:57,383][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:12:57,387][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:12:57,387][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:12:57,387][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:12:57,388][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:12:57,388][train][INFO] - Logging hyperparameters!
[2022-07-12 12:12:57,393][train][INFO] - Starting training!
[2022-07-12 12:12:57,393][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:12:57,401][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:12:57,401][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:12:57,401][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:13:17,750][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:13:17,922][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:13:23,238][train][INFO] - Starting testing!
[2022-07-12 12:13:23,759][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:13:24,112][train][INFO] - Finalizing!
[2022-07-12 12:14:07,939][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:14:07,940][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:14:08,032][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:14:10,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:14:13,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:14:13,337][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:14:13,337][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:14:13,339][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:14:13,343][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:14:13,344][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:14:13,344][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:14:13,344][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:14:13,345][train][INFO] - Logging hyperparameters!
[2022-07-12 12:14:13,349][train][INFO] - Starting training!
[2022-07-12 12:14:13,350][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:14:13,351][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:14:13,351][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:14:13,352][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:14:33,629][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:14:33,773][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:14:38,228][train][INFO] - Starting testing!
[2022-07-12 12:14:38,733][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:14:39,089][train][INFO] - Finalizing!
[2022-07-12 12:17:05,144][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:17:05,144][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:17:05,212][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:17:08,358][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:17:10,847][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:17:10,848][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:17:10,849][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:17:10,851][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:17:10,855][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:17:10,855][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:17:10,855][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:17:10,856][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:17:10,869][train][INFO] - Logging hyperparameters!
[2022-07-12 12:17:10,874][train][INFO] - Starting training!
[2022-07-12 12:17:10,875][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:17:10,875][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:17:10,876][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:17:10,876][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:17:31,248][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:17:31,390][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:17:36,099][train][INFO] - Starting testing!
[2022-07-12 12:17:36,607][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:17:36,963][train][INFO] - Finalizing!
[2022-07-12 12:18:41,419][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:18:41,419][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:18:41,486][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:18:43,763][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:18:46,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:18:46,029][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:18:46,030][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:18:46,031][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:18:46,035][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:18:46,036][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:18:46,036][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:18:46,036][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:18:46,037][train][INFO] - Logging hyperparameters!
[2022-07-12 12:18:46,041][train][INFO] - Starting training!
[2022-07-12 12:18:46,042][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:18:46,043][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:18:46,043][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:18:46,044][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:19:06,341][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:19:06,506][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:19:11,010][train][INFO] - Starting testing!
[2022-07-12 12:19:11,522][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:19:11,878][train][INFO] - Finalizing!
[2022-07-12 12:28:02,480][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:28:02,480][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:28:02,549][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:28:06,015][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:28:08,636][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:28:08,638][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:28:08,639][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:28:08,640][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:28:08,645][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:28:08,645][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:28:08,645][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:28:08,646][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:28:08,646][train][INFO] - Logging hyperparameters!
[2022-07-12 12:28:08,651][train][INFO] - Starting training!
[2022-07-12 12:28:08,651][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:28:08,652][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:28:08,653][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:28:08,653][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:28:29,310][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:28:29,454][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:28:34,366][train][INFO] - Starting testing!
[2022-07-12 12:28:34,879][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:28:35,236][train][INFO] - Finalizing!
[2022-07-12 12:29:20,173][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:29:20,173][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:29:20,243][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:29:22,925][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:29:25,197][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:29:25,199][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:29:25,200][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:29:25,202][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:29:25,206][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:29:25,206][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:29:25,206][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:29:25,207][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:29:25,207][train][INFO] - Logging hyperparameters!
[2022-07-12 12:29:25,212][train][INFO] - Starting training!
[2022-07-12 12:29:25,213][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:29:25,214][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:29:25,214][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:29:25,214][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:29:45,551][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:29:45,695][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:29:50,129][train][INFO] - Starting testing!
[2022-07-12 12:29:50,640][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:29:51,003][train][INFO] - Finalizing!
[2022-07-12 12:32:27,956][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:32:27,957][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:32:28,026][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:32:30,823][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:32:33,392][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:32:33,394][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:32:33,395][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:32:33,397][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:32:33,401][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:32:33,421][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:32:33,421][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:32:33,422][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:32:33,422][train][INFO] - Logging hyperparameters!
[2022-07-12 12:32:33,426][train][INFO] - Starting training!
[2022-07-12 12:32:33,427][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:32:33,428][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:32:33,428][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:32:33,429][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:32:53,830][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:32:53,973][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:34:01,888][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:34:01,904][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:34:01,976][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:34:04,373][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:35:59,457][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:35:59,459][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:35:59,550][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:36:01,712][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:36:04,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:36:04,261][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:36:04,262][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:36:04,264][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:36:04,268][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:36:04,268][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:36:04,268][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:36:04,269][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:36:04,269][train][INFO] - Logging hyperparameters!
[2022-07-12 12:36:04,274][train][INFO] - Starting training!
[2022-07-12 12:36:04,275][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:36:04,276][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:36:04,305][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:36:04,306][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:36:24,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:36:24,711][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:36:29,471][train][INFO] - Starting testing!
[2022-07-12 12:36:29,981][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:36:30,337][train][INFO] - Finalizing!
[2022-07-12 16:02:00,060][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:02:00,090][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:02:00,182][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:02:16,471][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:02:19,269][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:02:19,272][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:02:19,273][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:02:19,274][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:02:19,278][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:02:19,279][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:02:19,279][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:02:19,279][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:02:19,280][train][INFO] - Logging hyperparameters!
[2022-07-12 16:02:19,284][train][INFO] - Starting training!
[2022-07-12 16:02:19,285][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:02:19,286][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:02:19,287][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:02:19,287][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:02:40,551][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:02:40,694][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 16:02:46,849][train][INFO] - Starting testing!
[2022-07-12 16:02:47,365][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:02:47,754][train][INFO] - Finalizing!
[2022-07-12 16:07:33,219][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:07:33,219][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:07:33,287][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:07:34,955][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:07:37,177][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:07:37,179][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:07:37,180][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:07:37,182][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:07:37,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:07:37,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:07:37,187][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:07:37,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:07:37,187][train][INFO] - Logging hyperparameters!
[2022-07-12 16:07:37,192][train][INFO] - Starting training!
[2022-07-12 16:07:37,193][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:07:37,194][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:07:37,194][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:07:37,194][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:07:57,363][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:07:57,517][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 16:08:02,107][train][INFO] - Starting testing!
[2022-07-12 16:08:02,617][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:08:03,005][train][INFO] - Finalizing!
[2022-07-12 16:32:05,705][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:32:05,705][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:32:05,773][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:32:07,311][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:32:10,086][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:32:10,087][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:32:10,088][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:32:10,090][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:32:10,094][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:32:10,094][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:32:10,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:32:10,095][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:32:10,095][train][INFO] - Logging hyperparameters!
[2022-07-12 16:32:10,100][train][INFO] - Starting training!
[2022-07-12 16:32:10,101][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:32:10,102][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:32:10,102][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:32:10,102][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:32:30,012][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:32:30,221][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:33:51,558][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:33:51,559][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:33:51,627][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:33:53,148][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:33:55,901][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:33:55,903][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:33:55,904][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:33:55,906][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:33:55,910][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:33:55,910][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:33:55,910][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:33:55,911][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:33:55,911][train][INFO] - Logging hyperparameters!
[2022-07-12 16:33:55,916][train][INFO] - Starting training!
[2022-07-12 16:33:55,941][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:33:55,942][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:33:55,942][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:33:55,942][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:34:15,955][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:34:16,163][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:34:41,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:34:41,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:34:41,250][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:34:42,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:34:45,569][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:34:45,571][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:34:45,572][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:34:45,574][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:34:45,578][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:34:45,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:34:45,578][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:34:45,579][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:34:45,579][train][INFO] - Logging hyperparameters!
[2022-07-12 16:34:45,584][train][INFO] - Starting training!
[2022-07-12 16:34:45,585][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:34:45,586][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:34:45,586][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:34:45,586][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:35:05,438][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:35:05,647][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:35:25,906][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:35:25,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:35:25,976][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:35:27,499][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:35:30,248][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:35:30,250][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:35:30,251][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:35:30,253][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:35:30,257][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:35:30,257][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:35:30,257][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:35:30,258][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:35:30,258][train][INFO] - Logging hyperparameters!
[2022-07-12 16:35:30,263][train][INFO] - Starting training!
[2022-07-12 16:35:30,264][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:35:30,265][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:35:30,265][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:35:30,265][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:35:50,322][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:35:50,531][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:36:19,032][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:36:19,032][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:36:19,100][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:36:20,612][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:36:23,308][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:36:23,310][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:36:23,310][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:36:23,326][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:36:23,330][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:36:23,330][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:36:23,331][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:36:23,331][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:36:23,331][train][INFO] - Logging hyperparameters!
[2022-07-12 16:36:23,336][train][INFO] - Starting training!
[2022-07-12 16:36:23,337][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:36:23,338][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:36:23,338][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:36:23,338][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:36:43,276][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:36:43,482][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:37:04,634][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:37:04,634][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:37:04,703][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:37:06,225][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:37:08,958][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:37:08,960][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:37:08,960][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:37:08,962][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:37:08,966][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:37:08,967][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:37:08,967][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:37:08,967][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:37:08,968][train][INFO] - Logging hyperparameters!
[2022-07-12 16:37:08,972][train][INFO] - Starting training!
[2022-07-12 16:37:08,973][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:37:08,974][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:37:08,974][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:37:08,976][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:37:29,354][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:37:29,560][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:37:35,798][train][INFO] - Starting testing!
[2022-07-12 16:37:36,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:37:36,839][train][INFO] - Finalizing!
[2022-07-12 16:38:19,845][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:38:19,845][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:38:19,913][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:38:21,431][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:38:24,111][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:38:24,113][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:38:24,113][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:38:24,115][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:38:24,119][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:38:24,119][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:38:24,120][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:38:24,120][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:38:24,120][train][INFO] - Logging hyperparameters!
[2022-07-12 16:38:24,125][train][INFO] - Starting training!
[2022-07-12 16:38:24,126][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:38:24,127][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:38:24,127][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:38:24,127][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:38:44,122][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:38:44,329][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:38:50,414][train][INFO] - Starting testing!
[2022-07-12 16:38:50,923][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:38:51,444][train][INFO] - Finalizing!
[2022-07-12 16:45:11,292][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:45:11,294][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:45:11,382][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:45:12,910][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:45:15,587][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:45:15,589][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:45:15,589][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:45:15,591][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:45:15,595][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:45:15,596][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:45:15,596][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:45:15,596][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:45:15,597][train][INFO] - Logging hyperparameters!
[2022-07-12 16:45:15,601][train][INFO] - Starting training!
[2022-07-12 16:45:15,602][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:45:15,603][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:45:15,603][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:45:15,604][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:45:35,449][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:45:35,657][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:45:41,057][train][INFO] - Starting testing!
[2022-07-12 16:45:41,572][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:45:42,255][train][INFO] - Finalizing!
[2022-07-12 16:46:16,493][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:46:16,493][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:46:16,561][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:46:18,092][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:46:20,746][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:46:20,747][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:46:20,748][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:46:20,750][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:46:20,754][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:46:20,754][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:46:20,755][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:46:20,755][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:46:20,755][train][INFO] - Logging hyperparameters!
[2022-07-12 16:46:20,760][train][INFO] - Starting training!
[2022-07-12 16:46:20,761][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:46:20,762][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:46:20,762][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:46:20,762][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:46:40,570][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:46:40,777][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:46:46,093][train][INFO] - Starting testing!
[2022-07-12 16:46:46,599][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:46:47,288][train][INFO] - Finalizing!
[2022-07-12 16:47:35,853][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:47:35,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:47:35,920][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:47:37,454][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:47:40,159][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:47:40,161][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:47:40,162][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:47:40,164][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:47:40,168][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:47:40,168][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:47:40,168][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:47:40,196][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:47:40,196][train][INFO] - Logging hyperparameters!
[2022-07-12 16:47:40,201][train][INFO] - Starting training!
[2022-07-12 16:47:40,202][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:47:40,203][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:47:40,203][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:47:40,203][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:48:00,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:48:00,626][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:48:06,003][train][INFO] - Starting testing!
[2022-07-12 16:48:06,516][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:48:07,243][train][INFO] - Finalizing!
[2022-07-12 16:50:00,732][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:50:00,732][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:50:00,800][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:50:02,340][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:50:05,011][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:50:05,013][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:50:05,014][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:50:05,015][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:50:05,019][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:50:05,020][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:50:05,020][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:50:05,020][train][INFO] - Logging hyperparameters!
[2022-07-12 16:50:05,065][train][INFO] - Starting training!
[2022-07-12 16:50:05,067][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:50:05,068][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:50:05,068][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:50:05,068][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:50:25,025][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:50:25,228][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:56:48,203][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:56:48,225][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:56:48,344][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:57:08,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:57:12,025][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:57:12,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:57:12,028][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:57:12,029][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 16:57:12,032][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,033][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,033][train][INFO] - Logging hyperparameters!
[2022-07-12 16:57:12,034][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 16:57:12,035][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,036][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 16:57:12,037][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,037][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,038][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 16:57:12,038][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 16:57:12,041][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,042][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 16:57:12,044][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,045][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 16:57:12,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,046][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 16:57:12,158][train][INFO] - Starting training!
[2022-07-12 16:57:12,159][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 16:57:13,035][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 16:57:13,037][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 16:57:13,039][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 16:57:13,039][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 16:57:13,043][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 16:57:13,046][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 16:57:13,047][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,049][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:57:13,050][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,053][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,056][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,056][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,057][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,058][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,691][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 21:04:22,850][train][INFO] - Starting testing!
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:24,204][train][INFO] - Finalizing!
[2022-07-12 21:04:24,205][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_005.ckpt
[2022-07-12 23:03:12,531][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:03:12,532][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:03:12,629][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:03:32,701][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:03:35,631][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:03:35,632][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:03:35,633][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:03:35,634][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:03:35,636][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,637][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:03:35,637][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:03:35,637][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:03:35,638][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:03:35,638][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,638][train][INFO] - Logging hyperparameters!
[2022-07-12 23:03:35,642][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,643][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:03:35,645][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,646][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:03:35,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,656][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,651][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:03:35,769][train][INFO] - Starting training!
[2022-07-12 23:03:35,770][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:03:36,638][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:03:36,643][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:03:36,677][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,679][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:03:36,684][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,688][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,270][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 23:23:11,887][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:23:11,899][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:23:12,024][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:23:32,480][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:23:35,549][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:23:35,551][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:23:35,551][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:23:35,552][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:23:35,556][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,557][train][INFO] - Logging hyperparameters!
[2022-07-12 23:23:35,561][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,562][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:23:35,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,564][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:23:35,565][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,567][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:23:35,570][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,571][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:23:35,572][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,573][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:23:35,574][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,575][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:23:35,576][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,577][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:23:35,685][train][INFO] - Starting training!
[2022-07-12 23:23:35,686][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:23:36,563][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:23:36,565][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:23:36,568][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:23:36,572][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:23:36,574][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:23:36,576][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:23:36,578][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,585][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:23:36,586][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,588][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,588][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,592][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,594][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,594][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,252][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 23:43:18,434][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:44:26,401][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:44:26,403][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:44:26,544][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:44:47,950][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:44:51,037][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:44:51,039][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:44:51,039][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:44:51,041][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:44:51,044][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:44:51,044][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:44:51,045][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:44:51,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,046][train][INFO] - Logging hyperparameters!
[2022-07-12 23:44:51,046][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:44:51,048][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,049][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,049][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:44:51,050][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:44:51,052][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,052][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,053][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:44:51,053][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:44:51,057][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,058][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:44:51,060][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,061][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:44:51,185][train][INFO] - Starting training!
[2022-07-12 23:44:51,185][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:44:52,047][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:44:52,050][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:44:52,051][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:44:52,054][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:44:52,054][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:44:52,059][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:44:52,062][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:44:52,064][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:44:52,064][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,065][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:44:52,065][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,065][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,068][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,069][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,071][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,071][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,072][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,929][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 00:34:17,379][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 00:34:17,404][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 00:34:17,520][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 00:34:35,081][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 00:34:38,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 00:34:38,084][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 00:34:38,085][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 00:34:38,086][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 00:34:38,089][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 00:34:38,089][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 00:34:38,090][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 00:34:38,090][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,091][train][INFO] - Logging hyperparameters!
[2022-07-13 00:34:38,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,093][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 00:34:38,093][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,094][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 00:34:38,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,096][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 00:34:38,097][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,098][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 00:34:38,098][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,099][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 00:34:38,100][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,101][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 00:34:38,112][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,113][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 00:34:38,217][train][INFO] - Starting training!
[2022-07-13 00:34:38,218][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 00:34:39,094][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 00:34:39,095][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 00:34:39,097][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 00:34:39,099][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 00:34:39,100][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 00:34:39,102][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 00:34:39,114][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,117][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 00:34:39,119][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,121][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,123][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,124][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,125][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,126][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,588][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,677][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,690][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,692][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,701][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,695][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,683][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,682][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,691][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:49:27,499][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 10:49:27,545][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 10:49:27,695][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 10:49:50,847][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 10:49:54,595][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 10:49:54,597][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 10:49:54,598][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 10:49:54,600][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 10:49:54,604][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 10:49:54,604][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 10:49:54,604][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 10:49:54,605][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 10:49:54,605][train][INFO] - Logging hyperparameters!
[2022-07-13 10:49:54,610][train][INFO] - Starting training!
[2022-07-13 10:49:54,611][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 10:49:54,612][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 10:49:54,612][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 10:49:54,613][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 10:50:16,358][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 10:50:16,679][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 10:50:25,466][train][INFO] - Starting testing!
[2022-07-13 10:50:25,979][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 10:50:26,688][train][INFO] - Finalizing!
[2022-07-13 10:58:13,413][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 10:58:13,413][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 10:58:13,491][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 10:58:22,403][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:00:03,570][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:00:03,570][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:00:03,651][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:00:06,369][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:00:09,410][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:00:09,412][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:00:09,413][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:00:09,415][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:00:09,419][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:00:09,419][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:00:09,419][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:00:09,420][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:00:09,420][train][INFO] - Logging hyperparameters!
[2022-07-13 11:00:09,425][train][INFO] - Starting training!
[2022-07-13 11:00:09,426][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:00:09,427][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:00:09,427][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:00:09,427][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:00:29,930][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:00:30,139][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 11:02:18,219][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:02:18,219][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:02:18,290][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:02:19,835][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:02:22,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:02:22,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:02:22,451][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:02:22,452][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:02:22,457][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:02:22,477][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:02:22,478][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:02:22,478][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:02:22,478][train][INFO] - Logging hyperparameters!
[2022-07-13 11:02:22,483][train][INFO] - Starting training!
[2022-07-13 11:02:22,484][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:02:22,485][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:02:22,485][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:02:22,485][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:02:42,877][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:02:43,088][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 11:03:32,621][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:03:32,621][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:03:32,735][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:03:35,392][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:03:37,903][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:03:37,905][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:03:37,906][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:03:37,908][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:03:37,913][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:03:37,913][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:03:37,913][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:03:37,914][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:03:37,914][train][INFO] - Logging hyperparameters!
[2022-07-13 11:03:37,920][train][INFO] - Starting training!
[2022-07-13 11:03:37,921][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:03:37,922][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:03:37,922][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:03:37,922][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:03:58,146][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:03:58,290][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:04:04,186][train][INFO] - Starting testing!
[2022-07-13 11:04:04,691][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:04:05,075][train][INFO] - Finalizing!
[2022-07-13 11:05:11,757][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:05:11,757][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:05:11,825][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:05:13,429][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:05:15,457][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:05:15,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:05:15,460][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:05:15,462][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:05:15,466][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:05:15,466][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:05:15,466][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:05:15,467][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:05:15,467][train][INFO] - Logging hyperparameters!
[2022-07-13 11:05:15,472][train][INFO] - Starting training!
[2022-07-13 11:05:15,473][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:05:15,474][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:05:15,474][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:05:15,474][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:05:35,704][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:05:35,849][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:05:41,892][train][INFO] - Starting testing!
[2022-07-13 11:05:42,398][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:05:42,764][train][INFO] - Finalizing!
[2022-07-13 11:06:59,173][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:06:59,183][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:06:59,253][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:07:01,677][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:07:04,124][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:07:04,126][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:07:04,127][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:07:04,128][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:07:04,133][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:07:04,133][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:07:04,133][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:07:04,134][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:07:04,134][train][INFO] - Logging hyperparameters!
[2022-07-13 11:07:04,139][train][INFO] - Starting training!
[2022-07-13 11:07:04,140][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:07:04,141][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:07:04,141][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:07:04,141][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:07:24,484][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:07:24,628][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:09:37,864][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:09:37,865][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:09:37,932][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:09:39,641][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:10:10,965][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:10:10,965][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:10:11,034][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:10:13,716][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:10:16,861][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:10:16,863][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:10:16,864][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:10:16,865][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:10:16,870][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:10:16,870][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:10:16,870][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:10:16,871][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:10:16,871][train][INFO] - Logging hyperparameters!
[2022-07-13 11:10:16,877][train][INFO] - Starting training!
[2022-07-13 11:10:16,878][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:10:16,879][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:10:16,879][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:10:16,879][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:10:37,171][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:10:37,360][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:11:04,874][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:11:04,874][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:11:04,943][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:11:06,925][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:11:09,838][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:11:09,840][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:11:09,841][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:11:09,842][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:11:09,846][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:11:09,847][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:11:09,847][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:11:09,847][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:11:09,848][train][INFO] - Logging hyperparameters!
[2022-07-13 11:11:09,854][train][INFO] - Starting training!
[2022-07-13 11:11:09,881][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:11:09,882][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:11:09,882][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:11:09,883][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:11:29,978][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:11:30,167][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:12:55,279][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:12:55,280][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:12:55,349][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:12:56,921][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:12:59,894][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:12:59,896][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:12:59,897][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:12:59,898][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:12:59,902][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:12:59,903][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:12:59,903][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:12:59,903][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:12:59,904][train][INFO] - Logging hyperparameters!
[2022-07-13 11:12:59,910][train][INFO] - Starting training!
[2022-07-13 11:12:59,911][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:12:59,912][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:12:59,912][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:12:59,912][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:13:20,419][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:13:20,607][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:13:40,926][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:13:40,927][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:13:40,998][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:13:44,037][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:13:47,097][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:13:47,099][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:13:47,100][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:13:47,101][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:13:47,106][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:13:47,106][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:13:47,106][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:13:47,107][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:13:47,107][train][INFO] - Logging hyperparameters!
[2022-07-13 11:13:47,113][train][INFO] - Starting training!
[2022-07-13 11:13:47,114][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:13:47,115][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:13:47,115][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:13:47,115][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:14:07,301][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:14:07,488][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:14:50,260][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:14:50,261][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:14:50,329][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:14:51,862][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:14:54,826][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:14:54,828][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:14:54,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:14:54,845][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:14:54,849][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:14:54,849][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:14:54,849][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:14:54,850][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:14:54,850][train][INFO] - Logging hyperparameters!
[2022-07-13 11:14:54,856][train][INFO] - Starting training!
[2022-07-13 11:14:54,857][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:14:54,858][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:14:54,858][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:14:54,859][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:15:15,151][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:15:15,341][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:15:34,984][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:15:34,985][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:15:35,053][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:15:36,591][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:15:39,529][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:15:39,531][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:15:39,532][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:15:39,533][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:15:39,538][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:15:39,538][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:15:39,538][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:15:39,539][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:15:39,539][train][INFO] - Logging hyperparameters!
[2022-07-13 11:15:39,545][train][INFO] - Starting training!
[2022-07-13 11:15:39,546][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:15:39,547][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:15:39,547][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:15:39,548][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:15:59,561][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:15:59,749][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:16:18,708][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:16:18,708][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:16:18,775][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:16:20,337][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:16:23,309][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:16:23,311][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:16:23,311][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:16:23,313][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:16:23,317][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:16:23,318][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:16:23,318][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:16:23,318][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:16:23,319][train][INFO] - Logging hyperparameters!
[2022-07-13 11:16:23,324][train][INFO] - Starting training!
[2022-07-13 11:16:23,325][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:16:23,326][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:16:23,327][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:16:23,327][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:16:43,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:16:43,699][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 8.7 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
66.8 M    Trainable params
222 K     Non-trainable params
67.0 M    Total params
268.048   Total estimated model params size (MB)
[2022-07-13 11:17:02,144][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:17:02,144][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:17:02,213][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:17:04,740][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:17:07,720][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:17:07,722][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:17:07,723][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:17:07,724][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:17:07,728][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:17:07,729][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:17:07,729][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:17:07,729][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:17:07,730][train][INFO] - Logging hyperparameters!
[2022-07-13 11:17:07,735][train][INFO] - Starting training!
[2022-07-13 11:17:07,736][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:17:07,737][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:17:07,737][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:17:07,737][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:17:27,849][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:17:28,000][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 1.6 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.7 M    Trainable params
222 K     Non-trainable params
59.9 M    Total params
239.655   Total estimated model params size (MB)
[2022-07-13 11:17:55,264][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:17:55,265][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:17:55,334][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:17:56,868][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:17:59,857][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:17:59,859][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:17:59,859][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:17:59,861][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:17:59,865][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:17:59,866][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:17:59,866][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:17:59,867][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:17:59,867][train][INFO] - Logging hyperparameters!
[2022-07-13 11:17:59,873][train][INFO] - Starting training!
[2022-07-13 11:17:59,873][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:17:59,874][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:17:59,875][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:17:59,875][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:18:19,923][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:18:20,074][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:18:42,532][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:18:42,532][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:18:42,600][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:18:44,143][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:18:47,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:18:47,085][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:18:47,086][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:18:47,088][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:18:47,092][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:18:47,092][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:18:47,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:18:47,093][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:18:47,093][train][INFO] - Logging hyperparameters!
[2022-07-13 11:18:47,098][train][INFO] - Starting training!
[2022-07-13 11:18:47,099][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:18:47,100][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:18:47,100][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:18:47,101][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:19:07,329][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:07,480][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:19:19,016][train][INFO] - Starting testing!
[2022-07-13 11:19:19,539][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:19,921][train][INFO] - Finalizing!
[2022-07-13 11:19:29,486][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:19:29,486][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:19:29,554][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:19:32,252][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:19:35,510][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:19:35,512][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:19:35,513][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:19:35,515][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:19:35,519][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:19:35,519][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:19:35,519][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:19:35,520][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:19:35,520][train][INFO] - Logging hyperparameters!
[2022-07-13 11:19:35,525][train][INFO] - Starting training!
[2022-07-13 11:19:35,526][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:19:35,527][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:19:35,527][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:19:35,528][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:19:55,778][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:55,928][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:20:07,185][train][INFO] - Starting testing!
[2022-07-13 11:20:07,708][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:20:08,108][train][INFO] - Finalizing!
[2022-07-13 11:23:45,891][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:23:45,903][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:24:29,361][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:24:29,361][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:24:29,432][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:24:32,327][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:24:35,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:24:35,616][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:24:35,617][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:24:35,618][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:24:35,624][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:24:35,625][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:24:35,625][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:24:35,625][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:24:35,626][train][INFO] - Logging hyperparameters!
[2022-07-13 11:24:35,631][train][INFO] - Starting training!
[2022-07-13 11:24:35,632][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:24:35,633][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:24:35,633][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:24:35,633][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:24:55,750][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:24:55,900][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:25:07,160][train][INFO] - Starting testing!
[2022-07-13 11:25:07,678][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:25:08,252][train][INFO] - Finalizing!
[2022-07-13 11:25:38,489][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:25:38,489][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:25:38,559][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:25:40,438][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:25:43,368][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:25:43,370][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:25:43,371][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:25:43,372][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:25:43,378][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:25:43,378][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:25:43,378][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:25:43,379][train][INFO] - Logging hyperparameters!
[2022-07-13 11:25:43,731][train][INFO] - Starting training!
[2022-07-13 11:25:43,733][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:25:43,734][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:25:43,734][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:25:43,735][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:26:03,928][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:26:04,094][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:28:30,046][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:28:30,058][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:28:30,127][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:28:32,927][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:28:36,166][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:28:36,168][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:28:36,169][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:28:36,170][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:28:36,176][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:28:36,176][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:28:36,176][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:28:36,177][train][INFO] - Logging hyperparameters!
[2022-07-13 11:28:36,199][train][INFO] - Starting training!
[2022-07-13 11:28:36,201][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:28:36,202][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:28:36,202][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:28:36,207][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:28:56,243][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:28:56,430][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:34:00,167][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:34:00,191][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:34:00,261][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:34:03,766][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:34:07,152][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:34:07,154][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:34:07,155][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:34:07,156][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:34:07,162][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:34:07,162][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:34:07,163][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:34:07,163][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:34:07,163][train][INFO] - Logging hyperparameters!
[2022-07-13 11:34:07,169][train][INFO] - Starting training!
[2022-07-13 11:34:07,170][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:34:07,171][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:34:07,171][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:34:07,171][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:34:27,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:34:27,510][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:40:36,127][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:40:36,127][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:40:36,196][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:40:39,746][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:40:43,068][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:40:43,070][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:40:43,071][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:40:43,072][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:40:43,078][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:40:43,078][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:40:43,079][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:40:43,079][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:40:43,079][train][INFO] - Logging hyperparameters!
[2022-07-13 11:40:43,085][train][INFO] - Starting training!
[2022-07-13 11:40:43,086][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:40:43,087][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:40:43,087][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:40:43,087][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:41:03,468][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:41:03,619][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:41:50,436][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:41:50,436][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:41:50,506][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:41:52,159][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:41:55,091][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:41:55,093][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:41:55,093][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:41:55,095][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:41:55,101][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:41:55,101][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:41:55,102][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:41:55,103][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:41:55,103][train][INFO] - Logging hyperparameters!
[2022-07-13 11:41:55,108][train][INFO] - Starting training!
[2022-07-13 11:41:55,109][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:41:55,110][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:41:55,110][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:41:55,111][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:42:15,251][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:42:15,402][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:42:54,686][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:42:54,686][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:42:54,756][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:42:56,296][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:42:59,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:42:59,261][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:42:59,262][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:42:59,264][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:42:59,269][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:42:59,270][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:42:59,270][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:42:59,270][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:42:59,271][train][INFO] - Logging hyperparameters!
[2022-07-13 11:42:59,276][train][INFO] - Starting training!
[2022-07-13 11:42:59,277][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:42:59,278][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:42:59,278][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:42:59,279][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:43:19,326][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:43:19,478][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:43:36,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:43:36,570][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:43:36,640][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:43:39,454][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:43:42,564][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:43:42,565][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:43:42,566][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:43:42,568][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:43:42,574][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:43:42,574][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:43:42,574][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:43:42,575][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:43:42,575][train][INFO] - Logging hyperparameters!
[2022-07-13 11:43:42,580][train][INFO] - Starting training!
[2022-07-13 11:43:42,581][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:43:42,582][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:43:42,582][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:43:42,583][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:44:02,791][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:44:02,941][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:44:36,789][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:44:36,789][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:44:36,860][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:44:38,801][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:44:41,934][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:44:41,936][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:44:41,937][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:44:41,938][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:44:41,944][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:44:41,944][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:44:41,944][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:44:41,945][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:44:41,945][train][INFO] - Logging hyperparameters!
[2022-07-13 11:44:41,951][train][INFO] - Starting training!
[2022-07-13 11:44:41,952][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:44:41,953][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:44:41,953][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:44:41,953][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:45:02,033][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:45:02,185][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:46:55,451][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:46:55,452][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:46:55,522][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:46:57,143][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:47:17,596][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:47:17,596][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:47:17,669][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:47:19,744][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:51:38,591][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:51:38,613][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:51:38,722][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:51:57,916][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:52:16,227][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:52:16,227][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:52:16,303][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:52:18,938][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:52:22,978][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:52:22,980][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:52:22,981][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:52:22,982][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:52:22,989][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:52:22,989][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:52:22,989][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:52:22,990][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:52:22,990][train][INFO] - Logging hyperparameters!
[2022-07-13 11:52:22,997][train][INFO] - Starting training!
[2022-07-13 11:52:22,998][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:52:23,009][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:52:23,009][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:52:23,010][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:52:45,049][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:52:45,284][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:53:32,468][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:53:32,469][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:53:32,558][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:53:34,494][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:53:37,850][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:53:37,852][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:53:37,853][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:53:37,855][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:53:37,861][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:53:37,861][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:53:37,861][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:53:37,862][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:53:37,880][train][INFO] - Logging hyperparameters!
[2022-07-13 11:53:37,886][train][INFO] - Starting training!
[2022-07-13 11:53:37,887][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:53:37,889][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:53:37,889][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:53:37,889][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:53:58,518][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:53:58,674][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:54:52,338][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:54:52,338][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:54:52,411][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:54:54,064][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:54:57,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:54:57,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:54:57,451][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:54:57,453][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:54:57,459][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:54:57,460][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:54:57,460][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:54:57,460][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:54:57,461][train][INFO] - Logging hyperparameters!
[2022-07-13 11:54:57,467][train][INFO] - Starting training!
[2022-07-13 11:54:57,468][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:54:57,469][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:54:57,470][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:54:57,470][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:55:18,046][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:55:18,203][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:55:23,351][train][INFO] - Starting testing!
[2022-07-13 11:55:23,873][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:55:24,258][train][INFO] - Finalizing!
[2022-07-13 11:56:09,154][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:56:09,155][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:56:09,228][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:56:10,886][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:56:14,264][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:56:14,266][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:56:14,267][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:56:14,269][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:56:14,275][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:56:14,275][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:56:14,275][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:56:14,276][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:56:14,276][train][INFO] - Logging hyperparameters!
[2022-07-13 11:56:14,281][train][INFO] - Starting training!
[2022-07-13 11:56:14,282][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:56:14,283][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:56:14,284][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:56:14,284][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:56:34,964][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:56:35,120][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:56:39,880][train][INFO] - Starting testing!
[2022-07-13 11:56:40,411][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:56:40,796][train][INFO] - Finalizing!
[2022-07-13 11:57:04,915][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:57:04,917][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:57:04,990][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:57:06,653][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:57:10,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:57:10,029][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:57:10,029][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:57:10,031][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:57:10,037][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:57:10,037][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:57:10,038][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:57:10,038][train][INFO] - Logging hyperparameters!
[2022-07-13 11:57:10,105][train][INFO] - Starting training!
[2022-07-13 11:57:10,107][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:57:10,108][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:57:10,108][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:57:10,108][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:57:30,689][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:57:30,867][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:03:04,830][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:03:04,847][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:03:04,917][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:03:08,628][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:03:11,833][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:03:11,835][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:03:11,836][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:03:11,838][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:03:11,843][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:03:11,843][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:03:11,844][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:03:11,844][train][INFO] - Logging hyperparameters!
[2022-07-13 12:03:11,868][train][INFO] - Starting training!
[2022-07-13 12:03:11,870][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 12:03:11,871][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:03:11,871][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 12:03:11,871][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:03:31,623][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 12:03:31,773][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:04:13,861][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:04:13,862][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:04:13,931][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:04:15,447][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:04:18,325][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:04:18,327][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:04:18,328][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:04:18,330][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:04:18,335][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:04:18,335][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:04:18,336][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:04:18,336][train][INFO] - Logging hyperparameters!
[2022-07-13 12:04:18,358][train][INFO] - Starting training!
[2022-07-13 12:04:18,361][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 12:04:18,361][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:04:18,362][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 12:04:18,362][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:04:38,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 12:04:38,270][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:06:48,898][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:06:48,909][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:06:49,018][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:07:09,129][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:07:27,511][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 12:07:27,607][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,608][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 12:07:27,690][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,691][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 12:07:27,757][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,758][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 12:07:27,809][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 12:07:27,832][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,833][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 12:07:27,871][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,872][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 12:07:27,874][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:07:27,875][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:07:27,876][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:07:27,877][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 12:07:27,882][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,882][train][INFO] - Logging hyperparameters!
[2022-07-13 12:07:28,135][train][INFO] - Starting training!
[2022-07-13 12:07:28,136][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 12:07:28,517][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 12:07:28,634][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 12:07:28,725][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 12:07:28,792][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 12:07:28,818][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 12:07:28,834][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 12:07:28,873][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 12:07:28,873][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,874][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,876][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,877][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,880][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,882][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,883][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,029][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 15:25:01,341][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 15:25:01,359][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 15:25:01,498][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 15:25:17,995][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 15:25:20,831][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 15:25:20,833][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 15:25:20,834][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 15:25:20,836][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 15:25:20,841][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 15:25:20,842][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 15:25:20,842][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 15:25:20,842][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 15:25:20,843][train][INFO] - Logging hyperparameters!
[2022-07-13 15:25:20,848][train][INFO] - Starting training!
[2022-07-13 15:25:20,849][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 15:25:20,850][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 15:25:20,850][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 15:25:20,850][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 15:25:41,453][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:25:41,601][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 24.1 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 15:32:26,126][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 15:32:26,126][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 15:32:26,196][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 15:32:27,787][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 15:32:30,954][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 15:32:30,956][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 15:32:30,957][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 15:32:30,959][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 15:32:30,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 15:32:30,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 15:32:30,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 15:32:30,983][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 15:32:30,983][train][INFO] - Logging hyperparameters!
[2022-07-13 15:32:30,988][train][INFO] - Starting training!
[2022-07-13 15:32:30,989][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 15:32:30,990][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 15:32:30,990][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 15:32:30,990][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 15:32:50,922][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:32:51,072][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 15:32:55,842][train][INFO] - Starting testing!
[2022-07-13 15:32:56,353][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:32:56,736][train][INFO] - Finalizing!
[2022-07-13 22:05:53,164][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,186][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,205][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,401][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,885][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,885][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,887][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,911][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:12:53,145][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:12:53,165][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:12:53,319][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:13:08,227][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:13:11,928][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:13:11,973][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:13:11,974][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:13:11,976][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:13:11,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:13:11,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:13:11,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:13:11,983][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:13:11,983][train][INFO] - Logging hyperparameters!
[2022-07-13 23:13:11,988][train][INFO] - Starting training!
[2022-07-13 23:13:11,989][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:13:11,990][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:13:11,990][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:13:11,991][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:13:33,164][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:13:33,323][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 23:13:56,993][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:13:56,993][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:13:57,063][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:13:59,374][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:14:01,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:14:01,461][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:14:01,462][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:14:01,464][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:14:01,470][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:14:01,470][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:14:01,470][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:14:01,471][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:14:01,471][train][INFO] - Logging hyperparameters!
[2022-07-13 23:14:01,476][train][INFO] - Starting training!
[2022-07-13 23:14:01,477][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:14:01,478][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:14:01,478][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:14:01,478][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:14:21,914][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:14:22,060][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 655 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 23:14:48,161][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:14:48,162][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:14:48,233][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:14:50,225][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:14:52,299][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:14:52,301][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:14:52,302][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:14:52,304][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:14:52,310][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:14:52,310][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:14:52,310][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:14:52,311][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:14:52,311][train][INFO] - Logging hyperparameters!
[2022-07-13 23:14:52,316][train][INFO] - Starting training!
[2022-07-13 23:14:52,317][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:14:52,318][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:14:52,318][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:14:52,319][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:15:12,981][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:15:13,126][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 655 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 23:16:33,973][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:16:33,973][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:16:34,043][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:16:35,958][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:16:37,969][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:16:37,970][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:16:37,971][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:16:37,973][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:16:37,979][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:16:37,979][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:16:37,979][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:16:37,980][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:16:37,980][train][INFO] - Logging hyperparameters!
[2022-07-13 23:16:37,985][train][INFO] - Starting training!
[2022-07-13 23:16:37,986][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:16:37,987][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:16:37,987][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:16:37,987][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:16:58,126][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:16:58,269][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 4.1 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.202   Total estimated model params size (MB)
[2022-07-13 23:17:05,087][train][INFO] - Starting testing!
[2022-07-13 23:17:05,596][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:17:05,959][train][INFO] - Finalizing!
[2022-07-13 23:17:59,699][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:17:59,699][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:17:59,770][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:18:01,416][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:18:03,478][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:18:03,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:18:03,481][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:18:03,483][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:18:03,488][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:18:03,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:18:03,512][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:18:03,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:18:03,513][train][INFO] - Logging hyperparameters!
[2022-07-13 23:18:03,518][train][INFO] - Starting training!
[2022-07-13 23:18:03,519][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:18:03,520][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:18:03,520][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:18:03,520][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:18:23,868][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:18:24,011][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 4.1 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.202   Total estimated model params size (MB)
[2022-07-13 23:18:48,980][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:18:48,981][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:18:49,051][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:18:50,647][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:18:52,654][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:18:52,656][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:18:52,657][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:18:52,659][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:18:52,664][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:18:52,665][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:18:52,665][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:18:52,666][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:18:52,666][train][INFO] - Logging hyperparameters!
[2022-07-13 23:18:52,671][train][INFO] - Starting training!
[2022-07-13 23:18:52,671][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:18:52,672][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:18:52,673][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:18:52,673][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:19:12,910][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:19:13,052][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:19:17,491][train][INFO] - Starting testing!
[2022-07-13 23:19:18,001][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:19:18,349][train][INFO] - Finalizing!
[2022-07-13 23:20:08,039][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:20:08,040][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:20:08,109][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:20:09,646][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:20:11,650][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:20:11,652][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:20:11,653][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:20:11,655][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:20:11,661][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:20:11,661][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:20:11,661][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:20:11,662][train][INFO] - Logging hyperparameters!
[2022-07-13 23:20:11,698][train][INFO] - Starting training!
[2022-07-13 23:20:11,700][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:20:11,701][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:20:11,701][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:20:11,702][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:20:31,847][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:20:32,002][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:21:15,496][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:21:15,497][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:21:15,567][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:21:17,123][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:21:19,119][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:21:19,121][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:21:19,122][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:21:19,123][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:21:19,129][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:21:19,129][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:21:19,129][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:21:19,130][train][INFO] - Logging hyperparameters!
[2022-07-13 23:21:19,152][train][INFO] - Starting training!
[2022-07-13 23:21:19,154][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:21:19,155][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:21:19,155][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:21:19,155][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:21:39,268][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:21:39,403][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:22:08,465][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:22:08,466][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:22:08,535][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:22:10,092][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:22:12,087][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:22:12,089][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:22:12,089][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:22:12,091][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:22:12,097][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:22:12,097][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:22:12,097][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:22:12,098][train][INFO] - Logging hyperparameters!
[2022-07-13 23:22:12,120][train][INFO] - Starting training!
[2022-07-13 23:22:12,122][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:22:12,123][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:22:12,123][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:22:12,124][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:22:32,370][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:22:32,507][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:23:46,047][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:23:46,047][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:23:46,117][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:23:47,650][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:23:49,695][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:23:49,697][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:23:49,698][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:23:49,699][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:23:49,705][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:23:49,705][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:23:49,705][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:23:49,706][train][INFO] - Logging hyperparameters!
[2022-07-13 23:23:49,728][train][INFO] - Starting training!
[2022-07-13 23:23:49,730][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:23:49,731][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:23:49,731][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:23:49,731][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:24:09,829][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:24:09,967][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:24:38,515][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:24:38,516][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:24:38,591][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:24:40,119][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:24:42,106][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:24:42,108][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:24:42,109][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:24:42,111][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:24:42,116][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:24:42,117][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:24:42,117][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:24:42,118][train][INFO] - Logging hyperparameters!
[2022-07-13 23:24:42,139][train][INFO] - Starting training!
[2022-07-13 23:24:42,141][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:24:42,142][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:24:42,142][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:24:42,143][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:25:02,226][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:25:02,362][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:28:06,286][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:28:06,311][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:28:06,415][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:28:24,973][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:28:39,698][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:28:39,737][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,738][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 23:28:39,746][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,747][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 23:28:39,750][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:28:39,750][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:28:39,752][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 23:28:39,756][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,757][train][INFO] - Logging hyperparameters!
[2022-07-13 23:28:39,772][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,773][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 23:28:39,783][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,784][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 23:28:39,790][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,791][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 23:28:39,791][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,792][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 23:28:39,797][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,798][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 23:28:39,902][train][INFO] - Starting training!
[2022-07-13 23:28:39,903][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 23:28:40,743][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 23:28:40,748][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 23:28:40,781][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 23:28:40,791][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 23:28:40,791][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 23:28:40,793][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 23:28:40,799][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 23:28:40,802][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:28:40,802][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,802][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:28:40,803][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,804][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,809][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,810][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,811][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,811][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,812][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,441][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:44:34,687][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,687][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,705][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,978][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:51:18,883][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:51:18,899][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:51:19,044][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:51:23,269][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:51:25,749][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:51:25,751][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:51:25,751][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:51:25,753][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:51:25,759][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:51:25,759][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:51:25,759][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:51:25,760][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:51:25,760][train][INFO] - Logging hyperparameters!
[2022-07-13 23:51:25,765][train][INFO] - Starting training!
[2022-07-13 23:51:25,766][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:51:25,767][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:51:25,767][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:51:25,767][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:51:45,984][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:51:46,127][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:52:12,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:52:12,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:52:12,971][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:52:14,498][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:52:16,489][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:52:16,491][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:52:16,492][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:52:16,493][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:52:16,499][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:52:16,499][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:52:16,500][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:52:16,500][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:52:16,533][train][INFO] - Logging hyperparameters!
[2022-07-13 23:52:16,538][train][INFO] - Starting training!
[2022-07-13 23:52:16,539][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:52:16,540][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:52:16,540][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:52:16,540][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:52:36,578][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:52:36,724][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:54:01,409][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:54:01,409][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:54:01,479][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:54:03,001][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:54:05,003][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:54:05,005][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:54:05,006][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:54:05,008][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:54:05,014][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:54:05,014][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:54:05,014][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:54:05,015][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:54:05,015][train][INFO] - Logging hyperparameters!
[2022-07-13 23:54:05,020][train][INFO] - Starting training!
[2022-07-13 23:54:05,021][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:54:05,022][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:54:05,022][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:54:05,022][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:54:25,095][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:54:25,256][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:04:46,473][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:04:46,491][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:04:46,625][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:05:05,882][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:05:08,678][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:05:08,680][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:05:08,681][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:05:08,682][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:05:08,688][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:05:08,688][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:05:08,689][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:05:08,689][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:05:08,689][train][INFO] - Logging hyperparameters!
[2022-07-14 00:05:08,694][train][INFO] - Starting training!
[2022-07-14 00:05:08,695][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:05:08,696][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:05:08,696][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:05:08,697][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:05:29,880][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:05:30,040][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:12:18,527][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:12:18,528][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:12:18,598][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:12:22,684][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:12:25,190][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:12:25,206][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:12:25,207][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:12:25,209][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:12:25,214][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:12:25,215][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:12:25,215][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:12:25,215][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:12:25,216][train][INFO] - Logging hyperparameters!
[2022-07-14 00:12:25,220][train][INFO] - Starting training!
[2022-07-14 00:12:25,221][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:12:25,222][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:12:25,222][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:12:25,223][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:12:45,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:12:45,644][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:13:20,400][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:13:20,400][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:13:20,471][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:13:23,429][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:13:25,468][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:13:25,470][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:13:25,471][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:13:25,473][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:13:25,479][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:13:25,479][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:13:25,479][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:13:25,480][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:13:25,480][train][INFO] - Logging hyperparameters!
[2022-07-14 00:13:25,485][train][INFO] - Starting training!
[2022-07-14 00:13:25,486][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:13:25,505][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:13:25,505][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:13:25,506][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:13:45,928][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:13:46,073][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:17:25,542][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:17:25,563][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:17:25,633][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:17:28,111][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:17:30,280][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:17:30,282][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:17:30,283][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:17:30,284][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:17:30,290][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:17:30,291][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:17:30,291][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:17:30,291][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:17:30,292][train][INFO] - Logging hyperparameters!
[2022-07-14 00:17:30,297][train][INFO] - Starting training!
[2022-07-14 00:17:30,298][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:17:30,299][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:17:30,299][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:17:30,299][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:17:50,611][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:17:50,756][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:21:25,991][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:21:26,013][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:21:26,082][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:21:28,805][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:21:31,166][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:21:31,168][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:21:31,169][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:21:31,171][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:21:31,177][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:21:31,177][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:21:31,177][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:21:31,178][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:21:31,178][train][INFO] - Logging hyperparameters!
[2022-07-14 00:21:31,183][train][INFO] - Starting training!
[2022-07-14 00:21:31,184][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:21:31,185][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:21:31,185][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:21:31,185][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:21:51,469][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:21:51,612][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:23:07,237][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:23:07,237][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:23:07,308][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:23:09,739][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:23:11,956][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:23:11,958][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:23:11,958][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:23:11,985][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:23:11,990][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:23:11,991][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:23:11,991][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:23:11,991][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:23:11,992][train][INFO] - Logging hyperparameters!
[2022-07-14 00:23:11,996][train][INFO] - Starting training!
[2022-07-14 00:23:11,997][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:23:11,998][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:23:11,998][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:23:11,999][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:23:32,298][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:23:32,440][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:25:59,829][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:25:59,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:25:59,925][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:26:02,594][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:26:04,766][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:26:04,768][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:26:04,769][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:26:04,770][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:26:04,776][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:26:04,776][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:26:04,776][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:26:04,777][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:26:04,777][train][INFO] - Logging hyperparameters!
[2022-07-14 00:26:04,782][train][INFO] - Starting training!
[2022-07-14 00:26:04,783][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:26:04,784][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:26:04,784][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:26:04,785][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:26:25,100][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:26:25,242][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:28:19,881][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:28:19,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:28:19,975][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:28:22,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:28:24,962][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:28:24,964][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:28:24,965][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:28:24,966][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:28:24,972][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:28:24,972][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:28:24,972][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:28:24,973][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:28:24,973][train][INFO] - Logging hyperparameters!
[2022-07-14 00:28:24,978][train][INFO] - Starting training!
[2022-07-14 00:28:24,979][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:28:24,980][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:28:24,980][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:28:24,980][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:28:45,223][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:28:45,367][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:29:43,647][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:29:43,647][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:29:43,718][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:29:46,342][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:29:48,474][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:29:48,476][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:29:48,477][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:29:48,479][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:29:48,484][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:29:48,485][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:29:48,485][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:29:48,485][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:29:48,486][train][INFO] - Logging hyperparameters!
[2022-07-14 00:29:48,490][train][INFO] - Starting training!
[2022-07-14 00:29:48,491][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:29:48,492][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:29:48,492][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:29:48,493][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:30:08,771][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:30:08,915][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:30:16,325][train][INFO] - Starting testing!
[2022-07-14 00:30:16,831][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:30:17,184][train][INFO] - Finalizing!
[2022-07-14 00:30:57,006][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:30:57,006][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:30:57,114][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:30:59,350][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:31:01,634][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:31:01,635][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:31:01,636][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:31:01,638][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:31:01,658][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:31:01,659][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:31:01,659][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:31:01,659][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:31:01,660][train][INFO] - Logging hyperparameters!
[2022-07-14 00:31:01,664][train][INFO] - Starting training!
[2022-07-14 00:31:01,665][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:31:01,666][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:31:01,667][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:31:01,667][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:31:21,791][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:31:21,936][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:31:27,227][train][INFO] - Starting testing!
[2022-07-14 00:31:27,740][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:31:28,267][train][INFO] - Finalizing!
[2022-07-14 00:31:53,736][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:31:53,736][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:31:53,805][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:31:56,021][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:31:58,049][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:31:58,051][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:31:58,052][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:31:58,054][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:31:58,059][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:31:58,060][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:31:58,060][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:31:58,060][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:31:58,061][train][INFO] - Logging hyperparameters!
[2022-07-14 00:31:58,065][train][INFO] - Starting training!
[2022-07-14 00:31:58,066][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:31:58,067][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:31:58,068][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:31:58,068][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:32:18,304][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:32:18,448][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:32:23,737][train][INFO] - Starting testing!
[2022-07-14 00:32:24,242][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:32:24,592][train][INFO] - Finalizing!
[2022-07-14 00:33:08,272][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:33:08,273][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:33:08,343][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:33:10,750][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:33:13,169][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:33:13,171][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:33:13,172][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:33:13,173][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:33:13,179][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:33:13,179][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:33:13,180][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:33:13,180][train][INFO] - Logging hyperparameters!
[2022-07-14 00:33:13,227][train][INFO] - Starting training!
[2022-07-14 00:33:13,229][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:33:13,230][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:33:13,230][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:33:13,231][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:33:33,553][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:33:33,691][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:34:57,264][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:34:57,279][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:34:57,348][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:35:00,083][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:35:02,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:35:02,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:35:02,450][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:35:02,452][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:35:02,458][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:35:02,458][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:35:02,458][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:35:02,459][train][INFO] - Logging hyperparameters!
[2022-07-14 00:35:02,481][train][INFO] - Starting training!
[2022-07-14 00:35:02,483][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:35:02,484][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:35:02,484][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:35:02,484][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:35:22,597][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:35:22,736][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:44:10,018][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:44:10,041][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:44:10,164][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:44:28,323][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:44:42,466][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:44:42,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:44:42,481][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:44:42,482][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:44:42,486][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:44:42,486][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:44:42,494][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 00:44:42,494][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,495][train][INFO] - Logging hyperparameters!
[2022-07-14 00:44:42,557][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,558][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 00:44:42,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 00:44:42,730][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,731][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 00:44:42,733][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,734][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 00:44:42,754][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,755][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 00:44:42,818][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,819][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 00:44:42,834][train][INFO] - Starting training!
[2022-07-14 00:44:42,835][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 00:44:42,866][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,867][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 00:44:42,868][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 00:44:43,564][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 00:44:43,585][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 00:44:43,739][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 00:44:43,743][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 00:44:43,760][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 00:44:43,819][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,820][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,821][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,824][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,826][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,826][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,827][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,830][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,602][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:56:48,818][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:56:48,839][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:56:48,910][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:56:52,570][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:56:55,051][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:56:55,053][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:56:55,054][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:56:55,056][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:56:55,061][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:56:55,062][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:56:55,062][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:56:55,063][train][INFO] - Logging hyperparameters!
[2022-07-14 00:56:55,142][train][INFO] - Starting training!
[2022-07-14 00:56:55,144][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:56:55,145][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:56:55,145][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:56:55,145][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:57:15,488][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:57:15,623][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 400   
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.248   Total estimated model params size (MB)
[2022-07-14 01:02:07,457][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,460][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,471][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,473][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,937][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,974][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:08,061][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:06:29,765][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 01:06:29,766][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 01:06:29,866][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 01:06:49,468][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 01:06:52,007][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 01:06:52,009][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 01:06:52,009][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 01:06:52,011][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 01:06:52,015][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,016][train][INFO] - Logging hyperparameters!
[2022-07-14 01:06:52,019][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,020][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 01:06:52,020][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,021][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 01:06:52,023][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,023][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,024][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 01:06:52,024][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 01:06:52,025][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,026][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 01:06:52,027][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,028][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 01:06:52,031][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,032][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 01:06:52,116][train][INFO] - Starting training!
[2022-07-14 01:06:52,117][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 01:06:53,021][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 01:06:53,022][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 01:06:53,025][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 01:06:53,025][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 01:06:53,027][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 01:06:53,028][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 01:06:53,033][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 01:06:53,036][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 01:06:53,036][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,036][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 01:06:53,037][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,039][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,041][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,042][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,043][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,045][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,046][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,679][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 01:10:34,726][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,726][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,727][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,727][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,731][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,732][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,753][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,754][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:13:00,165][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 01:13:00,166][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 01:13:00,275][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 01:13:16,558][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 01:13:19,187][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 01:13:19,199][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,201][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,202][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,200][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,200][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 01:13:19,206][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 01:13:19,207][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 01:13:19,208][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 01:13:19,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 01:13:19,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 01:13:19,212][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 01:13:19,212][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 01:13:19,213][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 01:13:19,213][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,213][train][INFO] - Logging hyperparameters!
[2022-07-14 01:13:19,349][train][INFO] - Starting training!
[2022-07-14 01:13:19,350][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 01:13:20,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 01:13:20,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 01:13:20,218][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 01:13:20,218][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,219][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 01:13:20,221][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,221][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,184][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 11:17:09,003][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 11:17:09,024][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 11:17:09,140][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 11:17:20,363][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 11:17:23,516][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 11:17:23,520][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 11:17:23,521][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 11:17:23,522][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 11:17:23,528][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 11:17:23,528][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 11:17:23,528][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:17:23,529][train][INFO] - Logging hyperparameters!
[2022-07-14 11:17:23,731][train][INFO] - Starting training!
[2022-07-14 11:17:23,733][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 11:17:23,734][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 11:17:23,734][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 11:17:23,734][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 11:17:44,257][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 11:17:44,414][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 11:29:48,759][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 11:29:48,773][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 11:29:48,874][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 11:30:07,044][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 11:30:21,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 11:30:21,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 11:30:21,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 11:30:21,613][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 11:30:21,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 11:30:21,615][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 11:30:21,616][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 11:30:21,616][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,620][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 11:30:21,629][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 11:30:21,629][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 11:30:21,629][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,630][train][INFO] - Logging hyperparameters!
[2022-07-14 11:30:21,630][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 11:30:21,654][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,655][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 11:30:21,662][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,663][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 11:30:21,667][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,668][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 11:30:21,786][train][INFO] - Starting training!
[2022-07-14 11:30:21,787][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 11:30:22,583][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 11:30:22,583][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 11:30:22,591][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 11:30:22,638][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 11:30:22,659][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 11:30:22,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 11:30:22,669][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 11:30:22,676][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 11:30:22,676][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,676][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 11:30:22,679][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,679][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,680][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,682][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,439][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 12:13:01,833][train][INFO] - Starting testing!
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:03,011][train][INFO] - Finalizing!
[2022-07-14 12:13:03,012][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010.ckpt
[2022-07-14 13:34:51,128][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 13:34:51,153][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 13:34:51,292][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 13:35:10,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 13:35:13,560][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 13:35:13,565][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 13:35:13,566][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 13:35:13,568][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 13:35:13,577][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 13:35:13,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 13:35:13,578][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 13:35:13,578][train][INFO] - Logging hyperparameters!
[2022-07-14 13:35:13,670][train][INFO] - Starting training!
[2022-07-14 13:35:13,672][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 13:35:13,673][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 13:35:13,673][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 13:35:13,674][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 13:35:35,289][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 13:35:35,515][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 13:36:57,994][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 13:36:58,009][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 13:36:58,080][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 13:37:01,334][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 13:37:03,760][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 13:37:03,762][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 13:37:03,763][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 13:37:03,764][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 13:37:03,770][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 13:37:03,770][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 13:37:03,771][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 13:37:03,771][train][INFO] - Logging hyperparameters!
[2022-07-14 13:37:03,796][train][INFO] - Starting training!
[2022-07-14 13:37:03,798][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 13:37:03,799][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 13:37:03,799][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 13:37:03,799][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 13:37:24,161][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 13:37:24,297][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 16:45:07,304][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 16:45:07,324][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 16:45:07,445][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 16:45:25,825][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 16:45:28,317][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 16:45:28,319][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 16:45:28,320][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 16:45:28,321][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 16:45:28,324][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,325][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 16:45:28,325][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,326][train][INFO] - Logging hyperparameters!
[2022-07-14 16:45:28,329][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,330][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 16:45:28,330][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,331][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,331][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 16:45:28,332][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 16:45:28,334][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,335][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 16:45:28,335][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,336][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 16:45:28,348][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,349][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 16:45:28,452][train][INFO] - Starting training!
[2022-07-14 16:45:28,453][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 16:45:29,350][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 16:45:29,352][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 16:45:29,352][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,352][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,360][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,716][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 20:30:38,993][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 20:30:39,017][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 20:30:39,125][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 20:30:57,192][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 20:30:59,668][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 20:30:59,671][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 20:30:59,672][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 20:30:59,673][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 20:30:59,677][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 20:30:59,677][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 20:30:59,678][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 20:30:59,678][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,678][train][INFO] - Logging hyperparameters!
[2022-07-14 20:30:59,680][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,681][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,681][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 20:30:59,682][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 20:30:59,682][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,682][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,683][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 20:30:59,683][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,683][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 20:30:59,684][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 20:30:59,686][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,687][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 20:30:59,692][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,693][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 20:30:59,812][train][INFO] - Starting training!
[2022-07-14 20:30:59,813][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 20:31:00,682][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 20:31:00,683][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 20:31:00,684][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 20:31:00,684][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 20:31:00,685][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 20:31:00,688][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 20:31:00,694][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 20:31:00,702][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 20:31:00,702][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,702][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 20:31:00,703][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,703][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,705][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,709][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,190][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:23:56,930][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:23:56,968][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:23:57,117][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:24:09,301][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:24:12,175][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:24:12,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:24:12,179][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:24:12,180][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:24:12,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:24:12,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:24:12,187][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:24:12,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-15 01:24:12,187][train][INFO] - Logging hyperparameters!
[2022-07-15 01:24:12,193][train][INFO] - Starting training!
[2022-07-15 01:24:12,194][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 01:24:12,195][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:24:12,195][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 01:24:12,195][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:24:33,221][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:24:33,401][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:25:02,564][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:25:02,565][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:25:02,635][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:25:04,325][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:25:06,509][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:25:06,511][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:25:06,512][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:25:06,513][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:25:06,519][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:25:06,519][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:25:06,520][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:25:06,520][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-15 01:25:06,520][train][INFO] - Logging hyperparameters!
[2022-07-15 01:25:06,525][train][INFO] - Starting training!
[2022-07-15 01:25:06,526][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 01:25:06,527][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:25:06,527][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 01:25:06,527][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:25:26,715][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:25:26,858][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:25:32,458][train][INFO] - Starting testing!
[2022-07-15 01:25:32,965][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:25:33,316][train][INFO] - Finalizing!
[2022-07-15 01:27:49,080][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:27:49,095][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:27:49,210][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:28:07,017][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:28:20,690][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,700][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:28:20,787][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,791][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:28:20,808][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:28:20,854][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,855][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:28:21,130][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:28:21,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:28:21,132][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:28:21,134][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:28:21,138][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,139][train][INFO] - Logging hyperparameters!
[2022-07-15 01:28:21,156][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,157][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:28:21,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:28:21,222][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,223][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:28:21,627][train][INFO] - Starting training!
[2022-07-15 01:28:21,628][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:28:21,719][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:28:21,803][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:28:21,812][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:28:21,856][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:28:22,170][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:28:22,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:28:22,224][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,226][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:28:22,227][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,234][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,229][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,231][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,233][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,232][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,070][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:38:09,589][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,606][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,613][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,636][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,652][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,661][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,937][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:41:54,786][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:41:54,787][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:41:54,887][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:42:14,555][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:42:17,495][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:42:17,496][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:42:17,497][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:42:17,498][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:42:17,503][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:42:17,503][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,504][train][INFO] - Logging hyperparameters!
[2022-07-15 01:42:17,504][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:42:17,506][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,507][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:42:17,508][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,509][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:42:17,510][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,511][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:42:17,515][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,515][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:42:17,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:42:17,517][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,517][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:42:17,680][train][INFO] - Starting training!
[2022-07-15 01:42:17,681][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:42:18,505][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:42:18,508][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:42:18,510][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:42:18,512][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:42:18,516][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:42:18,519][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:42:18,521][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,528][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,517][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:42:18,538][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,525][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,538][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,522][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,518][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:42:18,539][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,539][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:42:18,539][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,105][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-15 01:43:24,862][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,862][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,863][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,865][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,867][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,877][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,883][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,927][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:45:33,594][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:45:33,599][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:45:33,723][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:45:48,346][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:45:50,557][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:45:50,579][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:45:50,579][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:45:50,581][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:45:50,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][train][INFO] - Logging hyperparameters!
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:45:50,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:45:50,701][train][INFO] - Starting training!
[2022-07-15 01:45:50,702][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:45:51,588][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:45:51,588][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:45:51,590][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:45:51,591][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:45:51,591][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,591][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,600][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,728][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-15 02:28:57,916][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,924][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,930][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,958][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,968][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,976][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,979][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,998][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:53:23,541][train][INFO] - Starting testing!
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:27,400][train][INFO] - Finalizing!
[2022-07-15 02:53:27,401][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v1.ckpt
[2022-07-15 06:28:21,751][train][INFO] - Starting testing!
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:23,253][train][INFO] - Finalizing!
[2022-07-15 06:28:23,253][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v2.ckpt
[2022-07-15 11:46:03,871][train][INFO] - Starting testing!
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:08,457][train][INFO] - Finalizing!
[2022-07-15 11:46:08,457][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v3.ckpt
[2022-07-15 12:35:15,132][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:35:15,152][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:35:15,267][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:35:34,054][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:35:37,098][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:35:37,101][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:35:37,102][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:35:37,103][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:35:37,109][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:35:37,109][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:35:37,109][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:35:37,110][train][INFO] - Logging hyperparameters!
[2022-07-15 12:35:37,155][train][INFO] - Starting training!
[2022-07-15 12:35:37,157][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:35:37,158][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:35:37,159][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:35:37,159][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:35:57,857][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:35:58,082][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:38:12,006][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:38:12,028][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:38:12,097][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:38:14,816][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:38:17,145][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:38:17,147][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:38:17,148][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:38:17,149][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:38:17,155][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:38:17,155][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:38:17,156][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:38:17,156][train][INFO] - Logging hyperparameters!
[2022-07-15 12:38:17,193][train][INFO] - Starting training!
[2022-07-15 12:38:17,195][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:38:17,196][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:38:17,197][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:38:17,197][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:38:37,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:38:37,592][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:41:39,725][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:41:39,725][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:41:39,794][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:41:41,810][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:41:43,871][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:41:43,872][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:41:43,873][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:41:43,875][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:41:43,880][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:41:43,881][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:41:43,903][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:41:43,903][train][INFO] - Logging hyperparameters!
[2022-07-15 12:41:43,925][train][INFO] - Starting training!
[2022-07-15 12:41:43,927][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:41:43,928][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:41:43,928][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:41:43,928][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:42:04,185][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:42:04,321][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:44:16,156][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:44:16,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:44:16,287][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:44:35,877][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:44:49,096][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:44:49,111][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:44:49,112][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:44:49,113][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 12:44:49,118][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,119][train][INFO] - Logging hyperparameters!
[2022-07-15 12:44:49,374][train][INFO] - Starting training!
[2022-07-15 12:44:49,375][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 12:44:49,615][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,616][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 12:44:49,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 12:44:49,747][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,749][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 12:44:49,750][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 12:44:49,973][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,974][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 12:44:49,975][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 12:44:50,202][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,203][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 12:44:50,206][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 12:44:50,211][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,212][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 12:44:50,212][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 12:44:50,306][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,307][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 12:44:50,310][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 12:44:50,332][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,334][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 12:44:50,334][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 12:44:50,339][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:44:50,339][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,339][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:44:50,340][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,340][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,341][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,345][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,571][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:54:01,758][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,759][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,773][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,777][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,788][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,800][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,834][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:02,009][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 13:02:32,787][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:02:32,805][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:02:32,912][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:02:38,173][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:03:13,632][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:03:13,632][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:03:13,702][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:03:15,221][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:03:17,197][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 13:03:17,199][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 13:03:17,200][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 13:03:17,201][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 13:03:17,207][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 13:03:17,207][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 13:03:17,208][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:03:17,209][train][INFO] - Logging hyperparameters!
[2022-07-15 13:03:17,308][train][INFO] - Starting training!
[2022-07-15 13:03:17,310][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 13:03:17,311][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 13:03:17,311][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 13:03:17,311][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 13:03:37,325][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 13:03:37,463][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 13:07:07,320][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:07:07,345][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:07:07,443][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:07:24,680][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:07:27,236][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 13:07:27,240][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,240][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 13:07:27,243][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,244][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 13:07:27,244][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,244][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 13:07:27,245][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 13:07:27,245][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 13:07:27,245][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,246][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 13:07:27,246][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 13:07:27,247][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 13:07:27,251][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 13:07:27,251][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 13:07:27,252][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][train][INFO] - Logging hyperparameters!
[2022-07-15 13:07:27,253][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 13:07:27,253][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 13:07:27,374][train][INFO] - Starting training!
[2022-07-15 13:07:27,375][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 13:07:28,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 13:07:28,241][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 13:07:28,245][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 13:07:28,245][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 13:07:28,247][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 13:07:28,253][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 13:07:28,254][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,264][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,266][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,266][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,267][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,269][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,272][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,740][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 14:13:08,786][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 14:13:08,801][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 14:13:08,889][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 14:13:27,457][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 14:13:30,064][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 14:13:30,065][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 14:13:30,066][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 14:13:30,067][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 14:13:30,070][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,071][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 14:13:30,071][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 14:13:30,071][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 14:13:30,072][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 14:13:30,072][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,072][train][INFO] - Logging hyperparameters!
[2022-07-15 14:13:30,073][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,074][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,081][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 14:13:30,081][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 14:13:30,081][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,082][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 14:13:30,082][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 14:13:30,083][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,084][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 14:13:30,188][train][INFO] - Starting training!
[2022-07-15 14:13:30,189][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 14:13:31,072][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 14:13:31,075][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 14:13:31,082][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 14:13:31,082][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 14:13:31,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 14:13:31,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 14:13:31,085][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 14:13:31,088][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 14:13:31,088][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,088][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 14:13:31,092][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,092][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,095][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,095][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,488][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 17:14:42,511][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,515][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,516][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,527][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,536][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,536][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,540][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,543][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-16 00:50:42,653][train][INFO] - Starting testing!
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:44,116][train][INFO] - Finalizing!
[2022-07-16 00:50:44,117][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v4.ckpt
[2022-07-16 01:46:46,124][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:46:46,132][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:46:46,282][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:47:01,455][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:47:01,456][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:47:01,526][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:47:13,107][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:47:15,944][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:47:15,946][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:47:15,947][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:47:15,949][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:47:15,955][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:47:15,955][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:47:15,955][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:47:15,956][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:47:15,956][train][INFO] - Logging hyperparameters!
[2022-07-16 01:47:15,961][train][INFO] - Starting training!
[2022-07-16 01:47:15,961][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:47:15,962][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:47:15,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:47:15,963][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:47:37,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:47:37,580][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:48:35,560][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:48:35,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:48:35,646][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:48:37,173][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:48:39,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:48:39,183][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:48:39,184][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:48:39,185][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:48:39,214][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:48:39,215][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:48:39,215][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:48:39,215][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:48:39,216][train][INFO] - Logging hyperparameters!
[2022-07-16 01:48:39,220][train][INFO] - Starting training!
[2022-07-16 01:48:39,221][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:48:39,222][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:48:39,222][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:48:39,223][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:48:59,376][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:48:59,518][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:49:19,293][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:49:19,293][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:49:19,378][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:49:20,908][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:49:22,923][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:49:22,925][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:49:22,926][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:49:22,928][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:49:22,933][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:49:22,934][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:49:22,934][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:49:22,935][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:49:22,935][train][INFO] - Logging hyperparameters!
[2022-07-16 01:49:22,939][train][INFO] - Starting training!
[2022-07-16 01:49:22,940][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:49:22,941][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:49:22,942][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:49:22,942][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:49:43,058][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:49:43,200][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:49:51,002][train][INFO] - Starting testing!
[2022-07-16 01:49:51,510][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:49:51,858][train][INFO] - Finalizing!
[2022-07-16 01:59:47,919][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:59:47,919][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:59:47,991][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:59:49,524][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:00:25,511][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:00:25,511][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:00:25,581][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:00:27,111][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:01:03,177][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:01:03,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:01:03,249][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:01:04,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:01:06,827][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:01:06,829][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:01:06,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:01:06,831][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:01:06,837][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:01:06,837][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:01:06,837][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:01:06,838][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:01:06,838][train][INFO] - Logging hyperparameters!
[2022-07-16 02:01:06,843][train][INFO] - Starting training!
[2022-07-16 02:01:06,844][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:01:06,845][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:01:06,845][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:01:06,845][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:01:27,013][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:01:27,156][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:02:07,174][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:02:07,175][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:02:07,248][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:02:08,772][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:02:10,795][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:02:10,797][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:02:10,797][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:02:10,799][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:02:10,805][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:02:10,805][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:02:10,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:02:10,806][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:02:10,806][train][INFO] - Logging hyperparameters!
[2022-07-16 02:02:10,811][train][INFO] - Starting training!
[2022-07-16 02:02:10,812][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:02:10,813][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:02:10,813][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:02:10,813][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:02:30,947][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:02:31,090][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:02:36,358][train][INFO] - Starting testing!
[2022-07-16 02:02:36,863][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:02:37,214][train][INFO] - Finalizing!
[2022-07-16 02:05:36,244][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:05:36,245][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:05:36,314][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:05:37,843][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:05:39,867][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:05:39,869][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:05:39,870][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:05:39,872][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:05:39,877][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:05:39,878][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:05:39,878][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:05:39,878][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:05:39,879][train][INFO] - Logging hyperparameters!
[2022-07-16 02:05:39,884][train][INFO] - Starting training!
[2022-07-16 02:05:39,885][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:05:39,886][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:05:39,886][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:05:39,886][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:06:00,113][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:06:00,257][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:09:16,933][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:09:16,933][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:09:17,003][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:09:18,522][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:09:20,531][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:09:20,533][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:09:20,533][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:09:20,535][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:09:20,541][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:09:20,541][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:09:20,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:09:20,564][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:09:20,564][train][INFO] - Logging hyperparameters!
[2022-07-16 02:09:20,569][train][INFO] - Starting training!
[2022-07-16 02:09:20,570][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:09:20,571][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:09:20,571][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:09:20,572][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:09:40,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:09:40,823][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:11:23,439][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:11:23,440][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:11:23,514][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:11:25,041][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:11:27,074][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:11:27,076][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:11:27,077][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:11:27,078][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:11:27,084][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:11:27,085][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:11:27,085][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:11:27,085][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:11:27,086][train][INFO] - Logging hyperparameters!
[2022-07-16 02:11:27,090][train][INFO] - Starting training!
[2022-07-16 02:11:27,091][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:11:27,092][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:11:27,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:11:27,093][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:11:47,298][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:11:47,442][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:13:32,496][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:13:32,496][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:13:32,567][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:13:34,095][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:13:36,114][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:13:36,116][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:13:36,117][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:13:36,119][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:13:36,125][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:13:36,125][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:13:36,125][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:13:36,126][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:13:36,126][train][INFO] - Logging hyperparameters!
[2022-07-16 02:13:36,131][train][INFO] - Starting training!
[2022-07-16 02:13:36,131][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:13:36,132][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:13:36,133][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:13:36,133][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:13:56,129][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:13:56,272][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:18:40,717][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:18:40,717][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:18:40,811][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:18:42,359][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:18:44,387][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:18:44,389][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:18:44,389][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:18:44,391][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:18:44,397][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:18:44,397][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:18:44,397][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:18:44,398][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:18:44,398][train][INFO] - Logging hyperparameters!
[2022-07-16 02:18:44,403][train][INFO] - Starting training!
[2022-07-16 02:18:44,404][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:18:44,405][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:18:44,405][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:18:44,405][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:19:04,576][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:19:04,720][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:20:36,322][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:20:36,323][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:20:36,394][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:20:37,923][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:20:39,948][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:20:39,950][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:20:39,951][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:20:39,953][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:20:39,959][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:20:39,959][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:20:39,959][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:20:39,960][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:20:39,982][train][INFO] - Logging hyperparameters!
[2022-07-16 02:20:39,987][train][INFO] - Starting training!
[2022-07-16 02:20:39,988][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:20:39,989][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:20:39,989][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:20:39,989][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:21:00,104][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:21:00,248][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:21:58,871][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:21:58,872][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:21:58,942][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:00,468][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:02,525][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:22:02,527][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:22:02,527][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:22:02,529][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:22:02,535][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:22:02,535][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:22:02,535][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:22:02,536][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:22:02,536][train][INFO] - Logging hyperparameters!
[2022-07-16 02:22:02,541][train][INFO] - Starting training!
[2022-07-16 02:22:02,542][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:22:02,543][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:22:02,543][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:22:02,543][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:22:10,517][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:22:10,517][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:22:10,587][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:12,125][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:48,490][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:22:48,490][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:22:48,562][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:50,094][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:52,152][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:22:52,154][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:22:52,155][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:22:52,156][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:22:52,162][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:22:52,162][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:22:52,162][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:22:52,163][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:22:52,163][train][INFO] - Logging hyperparameters!
[2022-07-16 02:22:52,168][train][INFO] - Starting training!
[2022-07-16 02:22:52,169][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:22:52,170][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:22:52,170][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:22:52,170][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:23:12,401][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:23:12,545][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:23:37,717][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:23:37,717][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:23:37,787][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:23:39,315][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:23:41,328][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:23:41,330][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:23:41,331][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:23:41,332][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:23:41,338][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:23:41,338][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:23:41,339][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:23:41,340][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:23:41,340][train][INFO] - Logging hyperparameters!
[2022-07-16 02:23:41,345][train][INFO] - Starting training!
[2022-07-16 02:23:41,346][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:23:41,347][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:23:41,347][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:23:41,348][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:24:01,542][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:24:01,686][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 12:52:49,415][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 12:52:49,437][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 12:52:49,544][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 12:52:56,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 12:52:59,500][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 12:52:59,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 12:52:59,505][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 12:52:59,506][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 12:52:59,512][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 12:52:59,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 12:52:59,513][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 12:52:59,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 12:52:59,513][train][INFO] - Logging hyperparameters!
[2022-07-16 12:52:59,518][train][INFO] - Starting training!
[2022-07-16 12:52:59,519][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 12:52:59,520][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 12:52:59,520][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 12:52:59,521][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 12:53:20,225][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 12:53:20,447][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 13:14:09,429][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 13:14:09,448][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 13:14:09,517][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 13:14:13,200][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 13:14:16,691][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 13:14:16,693][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 13:14:16,693][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 13:14:16,695][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 13:14:16,701][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 13:14:16,701][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 13:14:16,701][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 13:14:16,709][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 13:14:16,709][train][INFO] - Logging hyperparameters!
[2022-07-16 13:14:16,714][train][INFO] - Starting training!
[2022-07-16 13:14:16,715][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 13:14:16,716][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 13:14:16,716][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 13:14:16,716][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 13:14:36,793][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 13:14:36,930][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-16 13:15:00,639][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 13:15:00,639][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 13:15:00,710][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 13:15:02,687][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 13:15:05,002][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 13:15:05,004][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 13:15:05,005][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 13:15:05,007][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 13:15:05,013][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 13:15:05,013][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 13:15:05,013][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 13:15:05,021][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 13:15:05,021][train][INFO] - Logging hyperparameters!
[2022-07-16 13:15:05,026][train][INFO] - Starting training!
[2022-07-16 13:15:05,027][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 13:15:05,028][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 13:15:05,028][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 13:15:05,029][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 13:15:24,954][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 13:15:25,090][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:34:13,305][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:34:13,328][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:34:13,486][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:34:29,567][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:34:34,124][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:34:34,126][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:34:34,127][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:34:34,129][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:34:34,135][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:34:34,135][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:34:34,135][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:34:34,144][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:34:34,145][train][INFO] - Logging hyperparameters!
[2022-07-17 00:34:34,150][train][INFO] - Starting training!
[2022-07-17 00:34:34,151][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:34:34,152][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:34:34,153][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:34:34,153][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:34:55,580][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:34:55,759][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:35:17,834][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:35:17,834][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:35:17,906][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:35:19,557][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:35:22,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:35:22,085][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:35:22,086][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:35:22,088][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:35:22,094][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:35:22,094][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:35:22,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:35:22,104][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:35:22,104][train][INFO] - Logging hyperparameters!
[2022-07-17 00:35:22,109][train][INFO] - Starting training!
[2022-07-17 00:35:22,110][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:35:22,111][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:35:22,112][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:35:22,112][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:35:42,425][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:35:42,564][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:39:40,582][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:39:40,583][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:39:40,654][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:39:42,298][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:39:44,815][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:39:44,817][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:39:44,818][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:39:44,819][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:39:44,825][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:39:44,825][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:39:44,826][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:39:44,835][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:39:44,835][train][INFO] - Logging hyperparameters!
[2022-07-17 00:39:44,840][train][INFO] - Starting training!
[2022-07-17 00:39:44,841][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:39:44,842][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:39:44,843][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:39:44,843][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:40:05,191][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:40:05,331][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:41:11,194][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:41:11,195][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:41:11,267][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:41:12,916][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:41:15,451][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:41:15,453][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:41:15,454][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:41:15,456][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:41:15,462][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:41:15,462][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:41:15,462][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:41:15,471][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:41:15,472][train][INFO] - Logging hyperparameters!
[2022-07-17 00:41:15,477][train][INFO] - Starting training!
[2022-07-17 00:41:15,478][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:41:15,479][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:41:15,479][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:41:15,479][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:41:36,030][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:41:36,171][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:41:51,591][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:41:51,591][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:41:51,665][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:41:53,294][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:41:55,812][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:41:55,814][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:41:55,814][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:41:55,816][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:41:55,822][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:41:55,822][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:41:55,822][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:41:55,832][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:41:55,832][train][INFO] - Logging hyperparameters!
[2022-07-17 00:41:55,837][train][INFO] - Starting training!
[2022-07-17 00:41:55,838][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:41:55,844][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:41:55,844][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:41:55,844][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:42:16,416][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:42:16,556][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:47:39,932][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:47:39,932][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:47:40,004][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:47:41,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:47:44,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:47:44,180][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:47:44,180][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:47:44,182][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:47:44,188][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:47:44,188][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:47:44,189][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:47:44,198][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:47:44,198][train][INFO] - Logging hyperparameters!
[2022-07-17 00:47:44,203][train][INFO] - Starting training!
[2022-07-17 00:47:44,204][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:47:44,205][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:47:44,206][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:47:44,206][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:48:04,722][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:04,863][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:48:11,328][train][INFO] - Starting testing!
[2022-07-17 00:48:14,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:14,392][train][INFO] - Finalizing!
[2022-07-17 00:48:33,539][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:48:33,540][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:48:33,611][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:48:35,251][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:48:37,868][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:48:37,870][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:48:37,871][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:48:37,873][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:48:37,879][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:48:37,879][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:48:37,879][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:48:37,889][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:48:37,889][train][INFO] - Logging hyperparameters!
[2022-07-17 00:48:37,894][train][INFO] - Starting training!
[2022-07-17 00:48:37,895][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:48:37,896][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:48:37,896][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:48:37,896][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:48:58,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:58,671][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:49:17,907][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:49:17,908][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:49:17,980][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:49:19,627][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:49:22,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:49:22,183][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:49:22,184][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:49:22,185][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:49:22,191][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:49:22,192][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:49:22,192][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:49:22,201][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:49:22,202][train][INFO] - Logging hyperparameters!
[2022-07-17 00:49:22,207][train][INFO] - Starting training!
[2022-07-17 00:49:22,207][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:49:22,209][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:49:22,209][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:49:22,209][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:49:42,644][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:49:42,784][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:49:48,250][train][INFO] - Starting testing!
[2022-07-17 00:49:50,950][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:49:51,321][train][INFO] - Finalizing!
[2022-07-17 00:50:14,917][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:50:14,917][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:50:14,989][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:50:16,633][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:50:19,170][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:50:19,172][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:50:19,173][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:50:19,175][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:50:19,180][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:50:19,181][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:50:19,181][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:50:19,190][train][INFO] - Logging hyperparameters!
[2022-07-17 00:50:19,235][train][INFO] - Starting training!
[2022-07-17 00:50:19,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:50:19,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:50:19,239][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:50:19,239][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:50:39,853][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:50:40,021][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-08-23 15:24:49,895][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-08-23 15:24:49,895][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-08-23 15:24:50,069][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
